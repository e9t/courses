<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Courses</title><link href="http://lucypark.kr/courses/" rel="alternate"></link><link href="http://lucypark.kr/courses/feeds/2015-dm.atom.xml" rel="self"></link><id>http://lucypark.kr/courses/</id><updated>2015-05-08T09:00:00+09:00</updated><entry><title>Artificial neural networks</title><link href="http://lucypark.kr/courses/2015-dm/artificial-neural-networks.html" rel="alternate"></link><updated>2015-05-08T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-05-08:courses/2015-dm/artificial-neural-networks.html</id><summary type="html">&lt;h2&gt;So, is there a one-size-fits-all algorithm?&lt;/h2&gt;
&lt;p&gt;No!
The best algorithm depends on the characteristics of the trainin data.&lt;/p&gt;
&lt;p&gt;&lt;table class="table"&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;&lt;th&gt;Logistic Regression&lt;/th&gt;&lt;th&gt;Decision Tree&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Does it require variables to be normally distributed?&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Does it suffer multicollinearity issue?&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Does it do as well with categorical variables?&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Does it conduct variable selection without stepwise?&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Does it apply to sparse data?&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;http://www.quora.com/What-are-the-advantages-of-different-classification-algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;"There's no such thing as a free lunch"&lt;/h2&gt;
&lt;h3&gt;Bias-variance tradeoff&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bias-variance decomposition
http://scott.fortmann-roe.com/docs/BiasVariance.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Time-performance tradeoff&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>파이썬으로 영어와 한국어 텍스트 다루기</title><link href="http://lucypark.kr/courses/2015-dm/text-mining.html" rel="alternate"></link><updated>2015-04-10T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-04-10:courses/2015-dm/text-mining.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;We use Python 3 in this tutorial, but provide minimal guidelines for Python 2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;지난 시간 복습&lt;/h2&gt;
&lt;h3&gt;Terminologies&lt;/h3&gt;
&lt;p&gt;&lt;table class="table"&gt;&lt;tr&gt;&lt;th&gt;English&lt;/th&gt;&lt;th&gt;한국어&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Document&lt;/td&gt;&lt;td&gt;문서&lt;/td&gt;&lt;td&gt;-&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Corpus&lt;/td&gt;&lt;td&gt;말뭉치&lt;/td&gt;&lt;td&gt;A set of documents&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Token&lt;/td&gt;&lt;td&gt;토큰&lt;/td&gt;&lt;td&gt;Meaningful elements in a text such as words or phrases or symbols&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Morphemes&lt;/td&gt;&lt;td&gt;형태소&lt;/td&gt;&lt;td&gt;Smallest meaningful unit in a language&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;POS&lt;/td&gt;&lt;td&gt;품사&lt;/td&gt;&lt;td&gt;Part-of-speech (ex: Nouns)&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/p&gt;
&lt;h3&gt;Text analysis process&lt;/h3&gt;
&lt;p&gt;&lt;img src="images/text-process.png" width="200px"&gt;&lt;/p&gt;
&lt;p&gt;전처리는 아래의 세부 과정으로 다시 한 번 나뉜다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Load text&lt;/li&gt;
&lt;li&gt;Tokenize text (ex: stemming, morph analyzing)&lt;/li&gt;
&lt;li&gt;Tag tokens (ex: POS, NER)&lt;/li&gt;
&lt;li&gt;Token(Feature) selection and/or filter/rank tokens (ex: stopword removal, TF-IDF)&lt;/li&gt;
&lt;li&gt;...and so on (ex: calculate word/document similarities, cluster documents)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Useful Python Packages for Text Mining and NLP&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://nltk.org"&gt;NLTK&lt;/a&gt;: Provides modules for text analysis (mostly language independent)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;설치하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install nltk
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;주요기능&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.nltk.org/book/ch02.html"&gt;Text corpora&lt;/a&gt;: 특히, 이 튜토리얼에서는 아래의 두 가지 데이터가 필요하니 미리 다운 받아두자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gutenberg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;maxent_treebank_pos_tagger&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.nltk.org/api/nltk.tag.html"&gt;Word POS, NER classification&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.nltk.org/book/ch06.html"&gt;Document classification&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://konlpy.org"&gt;KoNLPy&lt;/a&gt;: Provides modules for Korean text analysis&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;설치하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install konlpy
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;주요기능&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://konlpy.org/en/latest/data/#corpora"&gt;Text corpora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://konlpy.org/en/latest/api/konlpy.tag/"&gt;Word POS classification&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Hannanum&lt;/li&gt;
&lt;li&gt;Kkma&lt;/li&gt;
&lt;li&gt;Mecab&lt;/li&gt;
&lt;li&gt;Komoran&lt;/li&gt;
&lt;li&gt;Twitter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http//radimrehurek.com/gensim/"&gt;Gensim&lt;/a&gt;: Provides modules for topic modeling and calculating similarities among documents&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;설치하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install -U gensim
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;주요기능&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Topic modeling&lt;ul&gt;
&lt;li&gt;&lt;a href="http://radimrehurek.com/gensim/models/ldamodel.html"&gt;Latent Dirichlet allocation (LDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://radimrehurek.com/gensim/models/lsimodel.html"&gt;Latent semantic indexing (LSI)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://radimrehurek.com/gensim/models/hdpmodel.html"&gt;Hierarchical Dirichlet process (HDP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Word embedding&lt;ul&gt;
&lt;li&gt;&lt;a href="radimrehurek.com/gensim/models/word2vec.html"&gt;word2vec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ryanmcgrath/twython"&gt;Twython&lt;/a&gt;: Provides easy access to Twitter API&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;설치하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pip install twython
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;사용예시: "Samsung (삼성)" 관련 트윗 받기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;twython&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Twython&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;settings&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;s&lt;/span&gt;    &lt;span class="c"&gt;# Create a file named settings.py, and put oauth KEY values inside&lt;/span&gt;
&lt;span class="n"&gt;twitter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Twython&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;APP_KEY&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;APP_SECRET&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OAUTH_TOKEN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OAUTH_TOKEN_SECRET&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tweets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;twitter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;삼성&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;user&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;screen_name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;created_at&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tweets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;statuses&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Text exploration&lt;/h2&gt;
&lt;h3&gt;1. Read document&lt;/h3&gt;
&lt;p&gt;이 튜토리얼에서는 NLTK, KoNLPy에서 제공하는 문서들을 사용한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;영어: &lt;a href="http://www.gutenberg.org/ebooks/158"&gt;Jane Austen의 소설 Emma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;한국어: &lt;a href="http://pokr.kr/bill/1809890"&gt;대한민국 국회 제 1809890호 의안&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;할 수 있는 사람은, 위의 문서 대신 다른 텍스트 데이터를 로딩하여 사용해보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;gutenberg&lt;/span&gt;   &lt;span class="c"&gt;# Docs from project gutenberg.org&lt;/span&gt;
&lt;span class="n"&gt;files_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gutenberg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fileids&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;      &lt;span class="c"&gt;# Get file ids&lt;/span&gt;
&lt;span class="n"&gt;doc_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gutenberg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;austen-emma.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;konlpy.corpus&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kobill&lt;/span&gt;    &lt;span class="c"&gt;# Docs from pokr.kr/bill&lt;/span&gt;
&lt;span class="n"&gt;files_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kobill&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fileids&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;         &lt;span class="c"&gt;# Get file ids&lt;/span&gt;
&lt;span class="n"&gt;doc_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kobill&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;1809890.txt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Tokenize&lt;/h3&gt;
&lt;p&gt;문서를 토큰으로 나누는 방법은 다양하다.
여기서는 영어에는 &lt;code&gt;nltk.regexp_tokenize&lt;/code&gt;, 한국어에는 &lt;code&gt;konlpy.tag.Twitter.morph&lt;/code&gt;를 사용해보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;regexp_tokenize&lt;/span&gt;
&lt;span class="n"&gt;pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;r&amp;#39;&amp;#39;&amp;#39;(?x) ([A-Z]\.)+ | \w+(-\w+)* | \$?\d+(\.\d+)?%? | \.\.\. | [][.,;&amp;quot;&amp;#39;?():-_`]&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;tokens_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;regexp_tokenize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doc_en&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pattern&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;konlpy.tag&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Twitter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Twitter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tokens_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;morphs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doc_ko&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Load tokens with &lt;code&gt;nltk.Text()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nltk.Text()&lt;/code&gt;는 문서 하나를 편리하게 탐색할 수 있는 다양한 기능을 제공한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="n"&gt;en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens_en&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean (For Python 2, &lt;code&gt;name&lt;/code&gt; has to be input as u'유니코드'. If you are using Python 2, use u'유니코드' for input of all following Korean text.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="n"&gt;ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens_ko&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;대한민국 국회 의안 제 1809890호&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c"&gt;# For Python 2, input `name` as u&amp;#39;유니코드&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지금부터 &lt;code&gt;nltk.Text()&lt;/code&gt;가 제공하는 다양한 기능을 하나씩 살펴보자.
(참고링크: &lt;a href="http://www.nltk.org/api/nltk.html#nltk.text.Text"&gt;class nltk.text.Text API 문서&lt;/a&gt;)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Tokens&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;       &lt;span class="c"&gt;# returns number of tokens (document length)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c"&gt;# returns number of unique tokens&lt;/span&gt;
&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;                  &lt;span class="c"&gt;# returns frequency distribution&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
191061
7927
FreqDist({',': 12018, '.': 8853, 'to': 5127, 'the': 4844, 'and': 4653, 'of': 4278, '"': 4187, 'I': 3177, 'a': 3000, 'was': 2385, ...})
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;       &lt;span class="c"&gt;# returns number of tokens (document length)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;  &lt;span class="c"&gt;# returns number of unique tokens&lt;/span&gt;
&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;                  &lt;span class="c"&gt;# returns frequency distribution&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
1707
476
FreqDist({'.': 61, '의': 46, '육아휴직': 38, '을': 34, '(': 27, ',': 26, '이': 26, ')': 26, '에': 24, '자': 24, ...})
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot frequency distributions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c"&gt;# Plot sorted frequency of top 50 tokens&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/fdist_en.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c"&gt;# Plot sorted frequency of top 50 tokens&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/fdist_ko.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: To save a plot programmably, and not through the GUI, overwrite &lt;code&gt;pylab.show&lt;/code&gt; with &lt;code&gt;pylab.savefig&lt;/code&gt; before drawing the plot (&lt;a href="http://stackoverflow.com/questions/27392390/how-do-i-send-nltk-plots-to-files"&gt;reference&lt;/a&gt;):
&lt;pre&gt;
from matplotlib import pylab
pylab.show = lambda: pylab.savefig('some_filename.png')
&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;: For those who see rectangles instead of letters in the saved plot file, include the following configurations before drawing the plot:
&lt;pre&gt;
from matplotlib import font_manager, rc
font_fname = 'c:/windows/fonts/gulim.ttc'     # A font of your choice
font_name = font_manager.FontProperties(fname=font_fname).get_name()
rc('font', family=font_name)
&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;Some example fonts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mac OS: &lt;code&gt;/Library/Fonts/AppleGothic.ttf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Count&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Emma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c"&gt;# Counts occurrences&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
865
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;초등학교&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c"&gt;# Counts occurrences&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
6
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dispersion plot&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dispersion_plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Emma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Frank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Jane&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/disp_en.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dispersion_plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;육아휴직&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;초등학교&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;공무원&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/disp_ko.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Concordance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concordance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Emma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
Displaying 5 of 865 matches:
                                     Emma by Jane Austen 1816 ] VOLUME I CHAPT
                                     Emma Woodhouse , handsome , clever , and 
both daughters , but particularly of Emma . Between &lt;em&gt;them&lt;/em&gt; it was more the int
 friend very mutually attached , and Emma doing just what she liked ; highly e
r own . The real evils , indeed , of Emma ' s situation were the power of havi
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean (or, use &lt;a href="http://konlpy.org/en/v0.4.3/api/konlpy/#konlpy.utils.concordance"&gt;konlpy.utils.concordance&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concordance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;초등학교&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
Displaying 6 of 6 matches:
 ․ 김정훈 김학송 의원 ( 10 인 ) 제안 이유 및 주요 내용 초등학교 저학년 의 경우 에도 부모 의 따뜻한 사랑 과 보살핌 이 필요 한
 을 할 수 있는 자녀 의 나이 는 만 6 세 이하 로 되어 있어 초등학교 저학년 인 자녀 를 돌보기 위해서 는 해당 부모님 은 일자리 를 
 다 . 제 63 조제 2 항제 4 호 중 “ 만 6 세 이하 의 초등학교 취학 전 자녀 를 ” 을 “ 만 8 세 이하 ( 취학 중인 경우 
 전 자녀 를 ” 을 “ 만 8 세 이하 ( 취학 중인 경우 에는 초등학교 2 학년 이하 를 말한 다 ) 의 자녀 를 ” 로 한 다 . 부 
 . ∼ 3 . ( 현행 과 같 음 ) 4 . 만 6 세 이하 의 초등학교 취 4 . 만 8 세 이하 ( 취학 중인 경우 학 전 자녀 를 양
세 이하 ( 취학 중인 경우 학 전 자녀 를 양육 하기 위하 에는 초등학교 2 학년 이하 를 여 필요하거 나 여자 공무원 이 말한 다 ) 의
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find similar words&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Emma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Frank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
she it he i harriet you her jane him that me and all they them there herself was hartfield be
mr mrs emma harriet you it her she he him hartfield them jane that isabella all herself look i me
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;자녀&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;similar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;육아휴직&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
논의
None
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Collocations&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collocations&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
Frank Churchill; Miss Woodhouse; Miss Bates; Jane Fairfax; Miss
Fairfax; every thing; young man; every body; great deal; dare say;
John Knightley; Maple Grove; Miss Smith; Miss Taylor; Robert Martin;
Colonel Campbell; Box Hill; said Emma; Harriet Smith; William Larkins
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collocations&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
초등학교 저학년; 육아휴직 대상
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more information on &lt;code&gt;nltk.Text()&lt;/code&gt;, see the &lt;a href="http://www.nltk.org/_modules/nltk/text.html#Text"&gt;source code&lt;/a&gt; or &lt;a href="http://www.nltk.org/api/nltk.html#nltk.text.Text"&gt;API&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Tagging and chunking&lt;/h2&gt;
&lt;p&gt;Until now, we used delimited text, namely &lt;em&gt;tokens&lt;/em&gt;, to explore our sample document.
Now let's classify words into given classes, namely &lt;em&gt;part-of-speech tags&lt;/em&gt;, and chunk text into larger pieces.&lt;/p&gt;
&lt;h3&gt;1. POS tagging&lt;/h3&gt;
&lt;p&gt;There are numerous ways of tagging a text.
Among them, the most frequently used, and developed way of tagging is arguably POS tagging.&lt;/p&gt;
&lt;p&gt;Since one document is too long to observe a parsed structure,
lets use one short sentence for each language.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;The little yellow dog barked at the Persian cat&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tags_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pos_tag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
[('The', 'DT'),
 ('little', 'JJ'),
 ('yellow', 'NN'),
 ('dog', 'NN'),
 ('barked', 'VBD'),
 ('at', 'IN'),
 ('the', 'DT'),
 ('Persian', 'NNP'),
 ('cat', 'NN')]
&lt;/pre&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is also possible to use the famous &lt;a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford"&gt;Stanford POS tagger with NLTK&lt;/a&gt;, with &lt;code&gt;from nltk.tag.stanford import POSTagger&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;konlpy.tag&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Twitter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Twitter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tags_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;작고 노란 강아지가 페르시안 고양이에게 짖었다&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
[('작고', 'Noun'),
 ('노란', 'Adjective'),
 ('강아지', 'Noun'),
 ('가', 'Josa'),
 ('페르시안', 'Noun'),
 ('고양이', 'Noun'),
 ('에게', 'Josa'),
 ('짖었', 'Noun'),
 ('다', 'Josa')]
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Noun phrase chunking&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.nltk.org/api/nltk.chunk.html#nltk.chunk.regexp.RegexpParser"&gt;&lt;code&gt;nltk.RegexpParser()&lt;/code&gt;&lt;/a&gt; is a great way to start chunking.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;English&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;parser_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RegexpParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;NP: {&amp;lt;DT&amp;gt;?&amp;lt;JJ&amp;gt;?&amp;lt;NN.*&amp;gt;*}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;chunks_en&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser_en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tags_en&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;chunks_en&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/tree_en.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Korean&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;parser_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RegexpParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;NP: {&amp;lt;Adjective&amp;gt;*&amp;lt;Noun&amp;gt;*}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;chunks_ko&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser_ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tags_ko&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;chunks_ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;draw&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src="images/tree_ko.png" width="700px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information on chunking, refer to &lt;a href="http://www.nltk.org/book/ch07.html"&gt;Extracting Information from Text&lt;/a&gt; for English, and &lt;a href="http://konlpy.org/en/v0.4.3/examples/chunking/"&gt;Chunking&lt;/a&gt; for Korean.&lt;/p&gt;
&lt;h2&gt;Drawing a word cloud&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;제 1809890호 의안의 빈도분포(frequency distribution)를 다시 살펴보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
FreqDist({'.': 61, '의': 46, '육아휴직': 38, '을': 34, '(': 27, ',': 26, '이': 26, ')': 26, '에': 24, '자': 24, ...})
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이 빈도분포의 data type과 attribute 목록을 확인해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
nltk.probability.FreqDist
&lt;/pre&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
['B',
 'N',
 ...
 'items',
 ...
 'pop',
 'popitem',
 'pprint',
 'r_Nr',
 'setdefault',
 'subtract',
 'tabulate',
 'unicode_repr',
 'update',
 'values']
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;items()&lt;/code&gt;를 사용하면 빈도분포의 item 전체를 set의 형태로 볼 수 있다. 이를 &lt;code&gt;data&lt;/code&gt;라는 이름의 변수에 저장한 후, data type을 관찰하자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ko&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;pre class="result"&gt;
dict_items([('명', 5), ('예상된', 3), ('하나', 1), ('11', 2), ('팀', 2), ...])
&lt;code&gt;&amp;lt;class 'dict_items'&amp;gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이 set을 이제 &lt;code&gt;words.csv&lt;/code&gt;라는 파일에 저장해보자. 데이터 header는 word,freq로 하면 된다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;words.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;word,freq&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;다음으로 아래의 코드를 복사하여 &lt;code&gt;words.csv&lt;/code&gt;가 있는 폴더 내에 &lt;code&gt;index.html&lt;/code&gt;라는 이름으로 저장하자.&lt;br&gt;
    &lt;script src="https://gist.github.com/e9t/e462f7462e9d83b03464.js?file=index.html" type="text/javascript"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;위와 같은 폴더에서 아래를 실행하자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="n"&gt;http&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="mi"&gt;8888&lt;/span&gt;      &lt;span class="c"&gt;# for Python2, `python -m SimpleHTTPServer`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;마지막으로, 모던 브라우저(ex: 크롬)의 주소창에 &lt;code&gt;http://localhost:8888&lt;/code&gt;를 입력하면 우리의 워드클라우드가 떠있을 것이다!&lt;br&gt;
    &lt;iframe src="http://bl.ocks.org/e9t/raw/e462f7462e9d83b03464/" width="600px" height="600px" frameborder=0&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;더 실험해보고 싶은 경우:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;위의 워드클라우드는 각종 특수문자, 조사 등도 포함되어 정보 전달력이 떨어진다. 워드클라우드에 명사만 표현되게 할 수 있을까?&lt;/li&gt;
&lt;li&gt;다른 임의의 문서로도 워드클라우드를 그릴 수 있나? (ex: 내 데이터마이닝 프로젝트 제안서) 해당 문서를 파이썬으로 읽고, 문서에서 높은 빈도로 등장한 단어를 추출 후, 워드클라우드로 그려보자.&lt;/li&gt;
&lt;li&gt;여러 개의 문서에 대한 워드클라우드를 그릴 수도 있나? 파이썬으로 여러 개의 문서를 한꺼번에 읽어들인 후, 높은 빈도로 등장한 단어를 추출해서 워드클라우드로 그려보자.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="text"></category><category term="lectures"></category></entry><entry><title>Regularization</title><link href="http://lucypark.kr/courses/2015-dm/regularization.html" rel="alternate"></link><updated>2015-03-25T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-25:courses/2015-dm/regularization.html</id><summary type="html">&lt;h2&gt;Overfitting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;변수를 줄이거나&lt;/li&gt;
&lt;li&gt;레코드 개수를 늘리거나&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regularization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$y = f(x) = b_0 + b_1x_1 + b_2x_2 + ... + b_mx_m + \epsilon$&lt;/li&gt;
&lt;li&gt;$\hat(y) = h(x) = b_0 + b_1x_1 + b_2x_2 + ... + b_mx_m$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Dimensionality reduction</title><link href="http://lucypark.kr/courses/2015-dm/dimensionality-reduction.html" rel="alternate"></link><updated>2015-03-21T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-21:courses/2015-dm/dimensionality-reduction.html</id><summary type="html">&lt;h2&gt;Variable selection in regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why should we select a subset of variables?&lt;ul&gt;
&lt;li&gt;May be expensive or not feasible to collect a full complement of predictors for future prediction.&lt;/li&gt;
&lt;li&gt;May be able to measure fewer predictors more accurately (e.g. in surveys)&lt;/li&gt;
&lt;li&gt;More predictors, more missing values&lt;/li&gt;
&lt;li&gt;Parsimony (a.k.a. Occam’s Razor): the simpler, the better.&lt;/li&gt;
&lt;li&gt;Multicollinearity: presence of two or more predictors sharing the same linear relationship with the outcome variables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Goal: 일반화 성능 향상&lt;ul&gt;
&lt;li&gt;More robust&lt;/li&gt;
&lt;li&gt;Higher predictive accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exhaustive search: 모든 변수 조합의 성능을 살펴봄&lt;ul&gt;
&lt;li&gt;${x_1}, {x_2}, {x_3}, ..., {x_1, x_2}, {x_2, x_3}, ..., {x_1, x_2, x_3}, ...$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial search&lt;ul&gt;
&lt;li&gt;Forward selection: 유의성이 큰 변수는 하나씩 더함&lt;ul&gt;
&lt;li&gt;${x_4} \to {x_4, x_5} \to {x_1, x_4, x_5} \to {x_1, x_3, x_4, x_5} \to {x_1, x_2, x_3, x_4, x_5}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backward elimination: 유의성이 작은 변수를 하나씩 제거&lt;ul&gt;
&lt;li&gt;${x_1, x_2, x_3, x_4, x_5} \to {x_1, x_3, x_4, x_5} \to {x_1, x_4, x_5} \to {x_4, x_5} to{x_4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stepwise selection: 유의성이 큰 변수를 더하거나 작은 변수를 하나씩 제거&lt;ul&gt;
&lt;li&gt;${x_4} \to {x_4, x_5} \to {x_1, x_4, x_5} \to {x_1, x_5} \to ...$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Recommendation systems</title><link href="http://lucypark.kr/courses/2015-dm/recommendation-systems.html" rel="alternate"></link><updated>2015-03-21T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-21:courses/2015-dm/recommendation-systems.html</id><summary type="html">&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;http://www.slideshare.net/kyuhwanjung/cvprml-20140111&lt;/li&gt;
&lt;li&gt;http://www.slideshare.net/xlos21/deview-recopick-aws?related=1&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Logistic regression</title><link href="http://lucypark.kr/courses/2015-dm/logistic-regression.html" rel="alternate"></link><updated>2015-03-20T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-20:courses/2015-dm/logistic-regression.html</id><summary type="html">&lt;h2&gt;오늘의 목표&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MLR의 블랙박스 열어보기&lt;/li&gt;
&lt;li&gt;로지스틱 회귀모형 개념 익히고 실제로 구현하기&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Multiple linear regression (revisited)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;지난 시간에 우리는 MLR(multiple linear regression)에 대해 배우고 파이썬으로 실제 데이터에 대해 모델을 구축해보았다&lt;/li&gt;
&lt;li&gt;MLR은 실제로 기술 모델링, 설명 모델링 모두를 위해 인기있는 방법론이니 잘 알아두면 좋다.&lt;/li&gt;
&lt;li&gt;그런데 우리 컴퓨터는 MLR을 어떻게 학습했을까? (i.e., 파라미터 $b_j$들을 어떻게 추정했을까?)&lt;/li&gt;
&lt;li&gt;이번 시간에는 MLR의 &lt;a href="http://en.wikipedia.org/wiki/Function_(mathematics)"&gt;블랙박스(black box)&lt;/a&gt; 안에 무엇이 있는지를 보자.&lt;br&gt;
    &lt;a href="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Function_machine2.svg/440px-Function_machine2.svg.png"&gt;&lt;img src="images/function.png" width="200px"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple linear regression? Fit a linear relationship between a quantitative dependent variable $y$ and a set of independent variables $x_1, x_2, ..., x_m$&lt;/p&gt;
&lt;p&gt;$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_mx_m + \epsilon$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$y$: output variable / dependent variable&lt;/li&gt;
&lt;li&gt;$x_j$: $j$th input variable / independent variable&lt;/li&gt;
&lt;li&gt;$b_j$: parameters / coefficients&lt;/li&gt;
&lt;li&gt;$m$: Number of input variables/features&lt;/li&gt;
&lt;li&gt;$\epsilon$: Observation noise&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ex: 33명 성인 여성의 나이와 수축혈압(SBP, systolic blood pressure)&lt;br&gt;
&lt;div class="row"&gt;
    &lt;div class="col-md-8"&gt;
    &lt;table class="table"&gt;&lt;tr&gt;&lt;th&gt;Age&lt;/th&gt;&lt;th&gt;SBP&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;22&lt;/td&gt;&lt;td&gt;131&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;23&lt;/td&gt;&lt;td&gt;128&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;116&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;106&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan="2" class="data-link"&gt;&lt;a href="data/sbp.csv"&gt;data link&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    &lt;/div&gt;
    &lt;div class="col-md-4"&gt;
    &lt;img src="images/sbp.png" width="300px"&gt;
    &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;OLS(ordinary least squares) for parameter estimation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MLR의 파라미터 $b_j$들을 추정하기 위해서는 OLS를 사용한다&lt;ul&gt;
&lt;li&gt;정답과의 오차를 최소화한다는 의미&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Formulation (단순화하기 위해 $b_0, b_1$만 고려해보자)&lt;ul&gt;
&lt;li&gt;Hypothesis: $h(x) = b_0 + b_1x_1$&lt;/li&gt;
&lt;li&gt;Parameters: $b_0, b_1$&lt;/li&gt;
&lt;li&gt;Cost function: $J(b_0, b_1) = \frac{1}{2n}\sum_{i=1}^{n}(h(x_1^{(i)}) - y^{(i)})^2$&lt;ul&gt;
&lt;li&gt;The equation above is called the MSE(mean squared error, 오차의 제곱의 평균)&lt;/li&gt;
&lt;li&gt;MSE 말고도 평균 오차(mean error), MAE, MAPE, RMSE 등을 사용할 수도 있다&lt;ul&gt;
&lt;li&gt;Mean error: $\frac{1}{n}\sum_{i=1}^{n} (h(x_1^{(i)}) - y^{(i)})$&lt;/li&gt;
&lt;li&gt;Mean absolute error (MAE): $\frac{1}{n}\sum_{i=1}^{n} |h(x_1^{(i)}) - y^{(i)}|$&lt;/li&gt;
&lt;li&gt;Mean absolute percentage error (MAPE): $100 \times \frac{1}{n}\sum_{i=1}^{n} |\frac{h(x_1^{(i)}) - y^{(i)}}{y^{(i)}}|$&lt;/li&gt;
&lt;li&gt;Root mean squared error (RMSE): $\sqrt{\frac{1}{2n}\sum_{i=1}^{n}(h(x_1^{(i)}) - y^{(i)})^2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;이 값은 작을수록 좋음. 즉, 좋은 MSE가 되게 하는 모델이 좋은 모델&lt;/li&gt;
&lt;li&gt;바꿔말하면 좋은 모델을 만들기 위해서는 작은 MSE가 되게 하면 됨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Goal: $arg \min J(b_0, b_1, ..., b_m)$&lt;ul&gt;
&lt;li&gt;즉, 비용 $J$를 최소화하는 $b_j (j=0, 1, ..., m)$를 찾자&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution&lt;ul&gt;
&lt;li&gt;Numerical solution: &lt;a href="http://en.wikipedia.org/wiki/Gradient_descent"&gt;Gradient descent&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Local minimum을 찾기 위한 방법론&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Analytical solution: Normal equation&lt;ul&gt;
&lt;li&gt;Let $y = X\beta$, then
    $$\beta = (X^TX)^{-1}X^Ty$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Add bias-variance decomposition with MSE --&gt;

&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ex: 33명 성인 여성의 나이와 심장동맥병(CD, coronary heart disease) 발병 여부&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CD==1&lt;/code&gt;: positive class (normally the minority class, 예측 대상, ex: 불량, 발병, 스팸 등)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CD==0&lt;/code&gt;: negative class (normally the majority class)&lt;br&gt;
&lt;div class="row"&gt;
    &lt;div class="col-md-8"&gt;
    &lt;table class="table"&gt;&lt;tr&gt;&lt;th&gt;Age&lt;/th&gt;&lt;th&gt;CD&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;22&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;23&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan="2" class="data-link"&gt;&lt;a href="data/cd.csv"&gt;data link&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
    &lt;/div&gt;
    &lt;div class="col-md-4"&gt;
    &lt;img src="images/cd.png" width="300px"&gt;
    &lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;위와 같이 일반적인 linear regression 알고리즘을 fitting한 후 $h(x) = ax+b$에 대해 다음과 같은 모델을 추가할 수 있다&lt;ul&gt;
&lt;li&gt;if $h(x) \geq 0.5$, then $\hat{y}=1$&lt;/li&gt;
&lt;li&gt;if $h(x) \lt 0.5$, then $\hat{y}=0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;하지만 이 방법은 두 가지 측면에서 적절하지 않다.&lt;ol&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Outlier"&gt;Outlier&lt;/a&gt;에 robust하지 못하다. 가령 나이가 1세이고 CD==0인 단 한 개의 점이 training set에 추가된다면 모델은 어떻게 바뀔까?&lt;/li&gt;
&lt;li&gt;실제로 $y$값은 0, 1의 두 가지 값밖에 가지지 못함에도 불구하고, $-\infty &amp;lt; h(x) &amp;lt; \infty$ 여서 $h(x)$가 0과 1 사이의 값 뿐 아니라 1보다 크거나 0보다 작은 값도 가질 수 있게 된다. (이 때, 분류 오차도 엄청 커질 수 있다)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logistic Regression? Fit a linear relationship between a &lt;em&gt;categorical&lt;/em&gt; dependent variable $y$ and a set of independent variables $x_1,x_2,...,x_m$&lt;ul&gt;
&lt;li&gt;주의: 이름에 등장하는 "regression"이라는 표현과는 달리 logisitic regression은 분류 문제를 풀기 위한 알고리즘!&lt;/li&gt;
&lt;li&gt;logistic regression = logit regression == maximum-entropy classification (MaxEnt) == log-linear classifier&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;목표: $0 \leq h(x) \leq 1$가 되는 $h(x)$를 만들어보자.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$h(x) = g(b_0 + b_1x_1 + ... + b_mx_m)$: Multiple linear regression에 함수 $g(x)$를 씌운 꼴&lt;/li&gt;
&lt;li&gt;where $g(z) = \frac{1}{1+e^{-z}}$ ("logistic function" or "&lt;a href="http://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid function&lt;/a&gt;")&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/logistic.png" width="300px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;MLE(maximum likelihood estimation) for parameter estimation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MLE를 알기 위해서는 먼저 likelihood의 개념을 아는 것이 중요하다.&lt;/li&gt;
&lt;li&gt;likelihood의 개념을 알기 위해서는 &lt;a href="https://sites.google.com/site/lucyparklab/4-discussions/bayesian"&gt;Bayesian 통계&lt;/a&gt;를 아는 것이 중요하다.&lt;ul&gt;
&lt;li&gt;$p(x \vert y)$: 사후확률(posterior)&lt;/li&gt;
&lt;li&gt;$p(y \vert x)$: 우도(likelihood)&lt;/li&gt;
&lt;li&gt;$p(x)$: 사전확률(prior)&lt;/li&gt;
&lt;li&gt;$p(y)$: 증거(evidence)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$p(x|y) = \frac{p(y|x)p(x)}{p(y)}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이 likelihood $p(y|x)$에 log를 씌우면 log likelihood가 되고 logistic regression에서는 log likelihood를 $w$에 대해 편미분하여 최대화하는 방식으로 파라미터를 구한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$L(w) = \sum_{i=1}^{n} \log{p(y|x)} = \sum \log{g(x, w)^{y}} + (1-g(x, w))^{1-y}$$&lt;/p&gt;
&lt;p&gt;where $g(x, w) = \frac{1}{1+\exp{-wx}}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;마지막으로 gradient decent를 사용하며 update rule은 $w \leftarrow w + \nu \sum(y-g(x,w))x$이다.&lt;/li&gt;
&lt;li&gt;식 유도 과정은 &lt;a href="http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/logistic-regression-note.pdf"&gt;이 문서&lt;/a&gt; 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;[Programming] Logistic regression with scikit-learn&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="../tips/terminal.html"&gt;먼저 시작하기 전에 터미널 프로그래밍을 위한 몇 가지 tip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;1. Data acquisition&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;오늘도 지난 시간처럼 데이터 파일을 직접 구해서 로딩하기보다는 편의를 위해 scikit-learn 패키지를 이용해보자.&lt;ul&gt;
&lt;li&gt;단, 이번에는 원래 scikit-learn 패키지에 포함되어 있는 데이터가 아니라 scikit-learn의 &lt;a href="http://scikit-learn.org/0.12/modules/generated/sklearn.datasets.fetch_mldata.html"&gt;&lt;code&gt;datasets.fetch_mldata()&lt;/code&gt;&lt;/a&gt; 메소드를 이용해서 &lt;a href="http://mldata.org"&gt;mldata.org&lt;/a&gt;에 있는 데이터셋을 받아올 것이다.&lt;/li&gt;
&lt;li&gt;그 밖에도 scikit-learn을 통해 접근하거나 다운로드 받을 수 있는 데이터셋에 관해서는 &lt;a href="http://scikit-learn.org/stable/datasets/"&gt;이 링크&lt;/a&gt;를 참고하자.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;이번에 우리가 사용할 데이터셋은 머신러닝계에서 아주 유명한 &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST 데이터&lt;/a&gt;이다.&lt;ul&gt;
&lt;li&gt;이 데이터셋은 너비 28픽셀, 높이 28픽셀로 된 숫자 필기 이미지 70,000장에 대한 데이터이다.&lt;/li&gt;
&lt;li&gt;오늘 풀어볼 문제는, "임의의 숫자 필기 이미지를 입력받았을 때 컴퓨터가 어떤 숫자에 대한 이미지인지 알게 할 수 있는가?" 즉, 필기 인식이다.&lt;/li&gt;
&lt;li&gt;사실은 사람들도 꾸준한 "학습(learning)"을 통해 생성된 "분류기"를 이용해 숫자를 분류한다.&lt;br&gt;
&lt;img src='images/mnist.gif'&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이제 데이터를 다운로드 받아보자. &lt;a href="http://scikit-learn.org/stable/datasets/#downloading-datasets-from-the-mldata-org-repository"&gt;&lt;code&gt;datasets.fetch_mldata()&lt;/code&gt;&lt;/a&gt; 메소드에
우리가 받고 싶은 데이터의 이름 'MNIST original'를 명시한 후,
데이터를 저장할 곳 &lt;code&gt;data_home&lt;/code&gt;을 현재 디렉토리(&lt;code&gt;.&lt;/code&gt;)로 정해주면 데이터가 다운로드 된 후 바로 변수 &lt;code&gt;d&lt;/code&gt;로 로드(load)된다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch_mldata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MNIST original&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_home&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data_home&lt;/code&gt;은 얼마든지 다른 디렉토리로 지정해줘도 된다.&lt;/li&gt;
&lt;li&gt;한번 데이터를 다운받고 나서 같은 명령어를 재실행하면 다운로드 없이 바로 데이터가 로드된다.&lt;/li&gt;
&lt;li&gt;이번에도 변수 &lt;code&gt;d&lt;/code&gt;는 dictionary 형태로 되어 있으며, &lt;a href="multiple-linear-regression.html#programming-slr-mlr-with-scikit-learn"&gt;지난 시간에 살펴보았던 당뇨병(diabetes) 데이터셋&lt;/a&gt;과 마찬가지로 dictionary를 구성하는 key, value 중 'data', 'target'라는 key를 비롯하여 'COL_NAMES', 'DESCR' 등의 key도 있다.
(이 key들이 어떤 value를 담고 있는지 궁금하다면 출력해보자.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;여기서는 곧바로 당뇨병 데이터와 마찬가지로 'data' key의 value를 &lt;code&gt;X&lt;/code&gt;에, 'target' key의 value를 &lt;code&gt;y&lt;/code&gt;라는 변수에 저장하고,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;     &lt;span class="c"&gt;# python shell에서 실행하는 경우 `print()`는 필요없음&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;의 차원(dimension)을 확인해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c"&gt;# check data type of variable X, y&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# returns (70000, 784) (70000,)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;출력된 결과를 보니 &lt;code&gt;X&lt;/code&gt;에는 70,000개의 record(row)와 784개의 attribute(column, variable)가 있고, &lt;code&gt;y&lt;/code&gt;는 70,000개의 record로 구성된 1차원 데이터라고 한다.&lt;/li&gt;
&lt;li&gt;또, 넘어가기 전에 &lt;strong&gt;X의 record수와 y의 record 수가 일치&lt;/strong&gt;하고 &lt;strong&gt;y는 1차원&lt;/strong&gt;임을 반드시 확인하고 넘어가자.  위의 두 가지 사항이 맞지 않는다면 꽤 골치 아파질 것이다.
(왜 그럴까? 각자 생각해보자.)&lt;/li&gt;
&lt;li&gt;한편, 여기서 사용하는 &lt;code&gt;shape&lt;/code&gt;는 numpy의 &lt;code&gt;ndarray&lt;/code&gt;나  pandas의 &lt;code&gt;DataFrame&lt;/code&gt; 등 특정 데이터 타입을 사용할 때만 적용할 수 있다는 점을 알아두면 좋다.
(즉, 파이썬의 &lt;code&gt;list&lt;/code&gt; 데이터 타입을 사용할 때는 &lt;code&gt;shape&lt;/code&gt;를 사용할 수 없는데, 무슨 말인지 모르겠으면 일단 넘어가자. 다음에 이런 일이 발생하게 된다면 어차피 에러가 뜰테니까.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. Data exploration&lt;/h3&gt;
&lt;p&gt;처음 데이터를 받고 나면 무엇부터 할 수 있을까?
바로 데이터 학습? 아니다.
다음은 실제 데이터를 다룰 떄 살펴보는 사항들이다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;데이터가 크기가 얼마나 되는가? (용량, 행/열 차원 수)&lt;/li&gt;
&lt;li&gt;$y$ 값으로 설정할만한 실수형 혹은 범주형 변수가 있는가?&lt;/li&gt;
&lt;li&gt;Missing data가 있는가?&lt;/li&gt;
&lt;li&gt;변수의 종류는 무엇인가? 실수형? 범주형?&lt;ul&gt;
&lt;li&gt;실수형은 정규화(normalization)해주는 것이 일반적&lt;/li&gt;
&lt;li&gt;범주형은 1-of-c 코딩 등의 방식으로 변환해주는 것이 일반적&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;데이터는 어떻게 분할할까?&lt;ul&gt;
&lt;li&gt;Training:Test=60:40이 일반적이기는 하지만 다른 방법은 없을까?&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)"&gt;10-fold cross validation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;우리도 본격적으로 logistic regression 알고리즘으로 분류 모델을 만들기 전에 우리가 가진 데이터를 이리저리 살펴보자.
Data exploration은 현재 가지고 있는 데이터를 이리저리 굴려가며 모양새를 확인하는 것이다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;X, y는 각각 어떤 값들로 이루어졌는지 확인해보기 위해 X, y 각각의 첫번째 값을 출력해보자. (배열의 첫번째 index는 1이 아니라 0이라는 사실이 기억나는가?)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;X는 특히 0과 256 미만의 숫자들로 구성이 되어 있는데 이것은 어떤 의미를 가질까? (256이라는 숫자가 힌트!)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;쉽게 이해하기 위해 &lt;code&gt;matplotlib&lt;/code&gt; 패키지를 이용해 첫 번째 record를 한 번 그려보자. (원래 matplotlib은 3rd party 패키지여서 따로 설치해야하지만, 다행히도 anaconda가 matplotlib을 미리 설치해줬다.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;X0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;           &lt;span class="c"&gt;# reshape 1*784 array to 28*28 array&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;image&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;binary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="c"&gt;# set runtime configurations (rc) for color maps&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                     &lt;span class="c"&gt;# plot matrix&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;X0.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;               &lt;span class="c"&gt;# save plot to image file&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;                          &lt;span class="c"&gt;# show plot on screen&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;무슨 모양 같은가? 그렇다. 숫자 '0'이다.&lt;br&gt;&lt;img src="images/X0.png" width="300px"&gt;&lt;/li&gt;
&lt;li&gt;그럼 &lt;code&gt;y[0]&lt;/code&gt;의 값을 출력해보면? 마찬가지로 '0'이다. &lt;code&gt;y[0]&lt;/code&gt;는 &lt;code&gt;X[0]&lt;/code&gt;의 "정답"이다. &lt;/li&gt;
&lt;li&gt;여유가 된다면 X, y의 첫번째 record가 아니라 42번째, 10000번째, 마지막 record 등도 그려보자.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이번에는 y가 어떤 값들을 가질 수 있는지 확인해보기 위해 다음을 입력해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c"&gt;# returns {0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.python.org/3/tutorial/datastructures.html#sets"&gt;&lt;code&gt;set()&lt;/code&gt;&lt;/a&gt;은 파이썬에서 제공하는 기본 함수로, 배열을 입력받으면 unique한 값들을 반환한다.&lt;ul&gt;
&lt;li&gt;예를들어, [1,2,3,2,2,1,1,1,3,1,1]이라는 배열을 입력받았을 때 [1,2,3]을 반환한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;우리가 하려는 것이 뭔지 대충 감이 잡히는가?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;우리는 &lt;code&gt;X&lt;/code&gt;에 손글씨로 숫자를 쓴 필기 이미지와, &lt;code&gt;y&lt;/code&gt;에 그 이미지의 label(정답)을 가지고 있으며 정답셋은 0-9까지의 숫자로 구성되어 있다.&lt;/li&gt;
&lt;li&gt;지난 시간에 다룬 당뇨병 데이터는 $y$가 실수형 벡터였던 것과 달리 이번 시간에 다루는 MNIST 데이터는 $y$가 범주형(categorical) 벡터이다. $y$가 실수형일 때는 regression problem이지만, $y$가 범주형일 때는 classification problem이다.&lt;/li&gt;
&lt;li&gt;보다시피 필기체 인식은, 가장 대표적인 분류 문제 중 하나이다.&lt;br&gt;
&lt;img src='images/diotek.jpg' width="300px"&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3. Data partitioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이번에는 데이터를 partitioning 해보자.&lt;ul&gt;
&lt;li&gt;다시 한 번 얘기하지만, data partitioning은 &lt;strong&gt;overfitting 방지, 일반화 성능 향상&lt;/strong&gt;을 위해 하는 것이다.&lt;/li&gt;
&lt;li&gt;Data partitioning의 의미를 잊었다면 &lt;a href="multiple-linear-regression.html#data-partitioning"&gt;이 곳&lt;/a&gt;에 가서 복습하고 오자.
 데이터로 컴퓨터를 학습시킬 때 가장 중요한 개념 중 하나이니 반드시 익혀둬야 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;당뇨병 데이터셋에서는 X, y를 단순한 list split을 통해 440개 record 중에서 400개를 training set, 40개를 test set으로 설정했다.
이번에는 record가 70,000개인데, 어떻게 나누는 것이 좋을까?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cross_validation&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;다시 list split을 사용해도 되지만, 이번에는 scikit-learn에서 제공하는 &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"&gt;&lt;code&gt;train_test_split()&lt;/code&gt;&lt;/a&gt;을 써보자.&lt;/li&gt;
&lt;li&gt;이 메소드는 데이터를 비율에 맞게 랜덤 샘플링(random sampling)해준다는 점에서 단순히 list split을 하는 것보다 바람직하다. 왜일까? &lt;code&gt;y&lt;/code&gt;의 값들이 오름차순으로 정렬되어 있기 때문이다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;test_size&lt;/code&gt; 파라미터는 test set의 비율이다. 0.4로 했으니 training set은 0.6이 될 것이고, 따라서 test set에는 70,000의 40%인 28,000개의 record가 random으로 selection돼서 들어간다. 이 값을 명시해주지 않으면 scikit-learn v0.15.2를 기준으로 test set이 0.25, training set이 0.75가 된다.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;random_state&lt;/code&gt;는 랜덤 샘플링할 때 사용하는 seed이다.
이 seed를 같은 숫자로 설정해주면 같은 결과를 얻게 되고, 설정해주지 않으면 실행할 때마다 다른 결과를 얻게 된다.
실험 재현(reproduction)의 측면에서 random number를 generation하는 seed를 지정해주는 것은 아주 중요하다.
여기서 사용한 1234가 아니라 51321, 0 등 어떤 숫자를 사용해도 되는데,
중요한 것은 실험 조작 등을 방지하고, 누구라도 같은 코드를 돌렸을 때 동일한 실험 결과를
낳을 수 있게 하기 위해 가급적이면 명시하는 것이 좋다.&lt;/li&gt;
&lt;li&gt;또, 여기서 사용한 서브패키지 &lt;code&gt;cross_validation&lt;/code&gt;은 사실 단순히 데이터를 두 개의 파티션으로 나누는 일 외에도 중요한 일을 하는데, 아직은 몰라도 되지만 곧 학기 중에 반드시 다루게 될 중요한 개념이니 이름을 익혀두도록 하자.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4. Training&lt;/h3&gt;
&lt;p&gt;자, 이제 드디어 로지스틱 회귀분석 모델을 학습해보자!
고맙고 신기하게도 scikit-learn 패키지를 이용하면 모델 학습은 단 세 줄만에 끝난다.
(물론 내부에는 누군가가 이미 고생해서 짜놓은 어마어마한 "black box"가 존재한다는 것을 잊지 말자.)
MLR을 학습할 때와 마찬가지로 LogisticRegression 클래스를 import한 후, instance를 생성하고, training set으로 모델을 학습하면 된다&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="c"&gt;# LR instance 생성&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;                        &lt;span class="c"&gt;# LR 학습&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;그런데... 결과가 뜨지 않는다.
컴퓨터가 먹통됐나? 아니다. 실제로 컴퓨터는 엄청 열심히 돌고 있다.&lt;/li&gt;
&lt;li&gt;당뇨병 데이터로 MLR을 돌릴 때는 뭔가 돌긴 돌았나 싶을 정도로 빨리 끝났지만,
이번에는 경우가 다르다.&lt;/li&gt;
&lt;li&gt;데이터 개수도 440개에서 70,000개로 늘어난데다가 무엇보다 차원의 수가 10개에서 784개로 늘어났다.&lt;ul&gt;
&lt;li&gt;데이터 크기가 10배가 증가했다고 해서 학습/처리 시간이 10배만큼 늘어나는 것이 아니라, 때로는 100배, 1000배씩 증가할 때도 있다.&lt;/li&gt;
&lt;li&gt;특히 차원의 개수가 늘어날 때 우리는 &lt;a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality"&gt;차원의 저주(curse of dimensionality)&lt;/a&gt;를 겪는다고 말한다.&lt;/li&gt;
&lt;li&gt;학습은 아니지만 계산 속도가 기하급수적으로 증가하는 간단한 실험: &lt;code&gt;sum(range(10**7))&lt;/code&gt;, &lt;code&gt;sum(range(10**8))&lt;/code&gt;, &lt;code&gt;sum(range(10**9)&lt;/code&gt;의 계산 속도를 비교해보자.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;쿼드 코어 노트북 기준으로 이 로지스틱 회귀분석 모델을 학습하는데는 약 45분이 걸렸다. (..)&lt;/li&gt;
&lt;li&gt;따라서 이 데이터 전체를 돌리는 것은 &lt;a href="assignments.html#assignment-1-classification"&gt;숙제&lt;/a&gt;다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;실제로 데이터로 좋은 모델은 학습 &lt;strong&gt;시간&lt;/strong&gt;이 짧고 &lt;strong&gt;성능(performance)&lt;/strong&gt;이 좋다.&lt;/p&gt;
&lt;p&gt;어떻게 하면 빠르게 학습시킬 수 있는가? 어떻게 하면 더 좋은 성능을 낼 수 있는가?
이런 질문들은 데이터마이너들에게 아주 익숙한 질문이다.&lt;/p&gt;
&lt;p&gt;그런데 여기서 끝내기엔 좀 아쉽지 않은가?
시간을 단축하기 위해 문제를 단순화 시키고, 하는 김에 csv에서 데이터 로딩하는 법을 익혀보자.&lt;/p&gt;
&lt;h3&gt;5. 2범주 MNIST: 0과 1 구분하기&lt;/h3&gt;
&lt;p&gt;0, 1은 프로그래밍할 때 굉장히 중요한 숫자들이다.
이들만 골라서 2범주 분류 문제를 한 번 풀어보자.
편의를 위해 &lt;a href="https://gist.github.com/e9t/5d4c2b48d8eca8c662ef#file-mnist_binarize-py"&gt;이 변환 파일&lt;/a&gt;을 이용해 10 class, 70000 row의 MNIST dataset을
2 class, 14780 row의 데이터셋으로 변환했다.&lt;/p&gt;
&lt;p&gt;먼저 데이터셋을 다운로드 받은 후, csv 파일을 열어 데이터 모양을 확인해보자.
(csv 파일은 엑셀로 열 수 있는 spreadsheet 형태의 데이터이며 comma-separated-values의 약자이다.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X: &lt;a href="data/mnist-x-bin.csv"&gt;lucypark.kr/courses/2015-dm/data/mnist-x-bin.csv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;y: &lt;a href="data/mnist-y-bin.csv"&gt;lucypark.kr/courses/2015-dm/data/mnist-y-bin.csv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;다음으로 아래 코드를 이용해 csv 데이터를 로딩해보자.&lt;/p&gt;
&lt;p&gt;여기서는 numpy의 &lt;a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html"&gt;&lt;code&gt;genfromtxt()&lt;/code&gt;&lt;/a&gt;를 써서 csv 파일을 로딩해보자.
파일에서 데이터 로딩하는 것도 그리 어렵지 않다!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;genfromtxt&lt;/span&gt;
&lt;span class="n"&gt;X_bin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mnist-x-bin.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_bin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;genfromtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mnist-y-bin.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;참고: pandas 패키지의 &lt;code&gt;read_csv()&lt;/code&gt;를 이용하면 더 간판하고 빠르게 csv 데이터를 읽을 수 있다.
pandas는 파이썬에서 DataFrame 등을 이용하여 구조적 데이터를 편리하게 분석할 수 있게 해주는 인기있는 도구이다. 관심있다면 검색해보자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;자, 이제 여러번 반복했듯 data partitioning, 그리고 model fitting(==training)을 해서 &lt;code&gt;lr2&lt;/code&gt;이라는 이름으로 저장하자.&lt;br /&gt;
(이 부분은 설명을 생략)&lt;/p&gt;
&lt;p&gt;모델을 학습한 다음에는 무엇을 해야할까?
성능을 측정하기 위해 시험을 봐야지! (Testing phase)
Training 데이터와 test 데이터 모두의 정확도를 도출해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_bin_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_bin_train&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_bin_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_bin_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;사실 분류 문제는 정확도 말고도 precision, recall, f-measure 등의 지표가 있다.
아래는 이들 지표에 대한 목록이다. (참고: &lt;a href="http://en.wikipedia.org/wiki/Precision_and_recall"&gt;Precision and recall&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img src="images/cmt.png" width="400px"&gt;
&lt;img src="images/cm.png"&gt;&lt;/p&gt;
&lt;p&gt;이 값들은 &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"&gt;precision_recall_fscore_support()&lt;/a&gt;을 이용해서 구할 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;precision_recall_fscore_support&lt;/span&gt;
&lt;span class="n"&gt;y_bin_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_bin_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;precision_recall_fscore_support&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_bin_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_bin_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;macro&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;성능 지표의 값들은 얼마가 나오는가?
괜찮은가?&lt;/p&gt;
&lt;h3&gt;6. 1범주 MNIST: 1-against-all&lt;/h3&gt;
&lt;p&gt;사실 우리가 애초에 풀려던 문제는 좀 어려운 문제였을 수 있다.
무려 10가지 숫자(digit) 중에서 맞는 숫자를 골라보라니!
(실제로 &lt;a href="http://www.livescience.com/38310-ai-has-four-year-old-iq.html"&gt;2013년 기준 현대 AI의 수준은 4살짜리 꼬마의 지능과 유사하다&lt;/a&gt;고 한다)&lt;/p&gt;
&lt;p&gt;이번에는 처음의 70000행의 MNIST 데이터셋을 그대로 사용하되
문제를 좀 단순화시켜서, 이미지에 등장한 숫자가 1인지 아닌지만 판단하는 모델 &lt;code&gt;lr1&lt;/code&gt;을 만들어보자.
즉, 이 모델은 0, 2, 3, 4, ..., 9는 0(또는 False)을 반환하고, 1만 1(또는 True)을 반환하면 된다.
이렇게.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="n"&gt;lr1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lr1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;이제 accuracy, precision, recall, f-measure을 재보자.
성능이 어떤가? 학습 시간은?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;참고:&lt;br&gt;
1. &lt;a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/"&gt;Visualizing MNIST: An Exploration of Dimensionality Reduction&lt;/a&gt;&lt;br&gt;
2. &lt;a href="http://deeplearning.net/tutorial/logreg.html"&gt;Classifying MNIST digits using Logistic Regression&lt;/a&gt;&lt;br&gt;
3. &lt;a href="https://www.kaggle.com/c/digit-recognizer"&gt;Kaggle Digit Recognizer contest&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary></entry><entry><title>Multiple linear regression</title><link href="http://lucypark.kr/courses/2015-dm/multiple-linear-regression.html" rel="alternate"></link><updated>2015-03-16T18:37:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-13:courses/2015-dm/multiple-linear-regression.html</id><summary type="html">&lt;h2&gt;오늘의 목표&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;데이터마이닝 알고리즘의 분류&lt;/li&gt;
&lt;li&gt;Data partitioning 개념 익히기&lt;/li&gt;
&lt;li&gt;그 외 데이터마이닝에서 빈번히 다루는 용어들 익히기&lt;/li&gt;
&lt;li&gt;단변수, 다변수 선형회귀분석(linear regression)의 개념 익히고 파이썬으로 돌려보기&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data mining process&lt;/h2&gt;
&lt;p&gt;&lt;img src="images/process.png" width="500px"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Problem definition&lt;/li&gt;
&lt;li&gt;Data acquisition and selection&lt;/li&gt;
&lt;li&gt;Data exploration&lt;/li&gt;
&lt;li&gt;Data preprocessing&lt;/li&gt;
&lt;li&gt;Train and evaluate data mining model&lt;/li&gt;
&lt;li&gt;Interpret results&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;목적에 따른 분류: Predictive methods vs Descriptive methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Predictive modeling&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Predict the &lt;em&gt;future&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Identify strong links between variables of data&lt;/li&gt;
&lt;li&gt;To predict the unknown consequence (dependent variable) based on the information provided (independent variable)&lt;ul&gt;
&lt;li&gt;과거를 통해 현재를 알고 미래를 내다보자&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Descriptive modeling&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Look back to the &lt;em&gt;past&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;To extract compact and easily understood information from large, sometimes gigantic databases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/methods.png" width="600px"&gt;&lt;/p&gt;
&lt;h3&gt;학습데이터에 따른 분류: Supervised learning vs Unsupervised learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;교사학습(Supervised learning)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Goal: predict a single "target" or "outcome" variable $y$&lt;ul&gt;
&lt;li&gt;입출력(input $X$-output$y$)의 쌍으로 구성된 training data로부터 입력을 출력을 사상하는 함수를 학습하는 과정&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Method: &lt;em&gt;Learn&lt;/em&gt; on training data, &lt;em&gt;score&lt;/em&gt; on test data&lt;ul&gt;
&lt;li&gt;즉, 입력벡터를 $X$, 그에 대응하는 출력벡터(i.e., label)를 $y$라고 할 때, training data는 $D={(x, y)}$로 주어지게 되며, 모델은 이 training data에 기반하여 관측하지 않은 새로운 데이터 $x'$가 들어왔을 때 그에 해당되는 label, $y'$을 추론하는 방법을 배우게 된다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ex: 분류(classification)와 회귀분석(regression)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;비교사학습(Unsupervised learning)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Goal: Explore intrinsic characteristics of data $X$&lt;/li&gt;
&lt;li&gt;Method: Estimate underlying distributions and/or segment data into meaningful groups or detect patterns&lt;ul&gt;
&lt;li&gt;There is no target (outcome) variable to predict or classify&lt;/li&gt;
&lt;li&gt;출력값 없이 오직 입력값만 주어지며, 이러한 입력값들의 공통적인 특성을 파악하여 학습하는 과정&lt;/li&gt;
&lt;li&gt;Training data는 $D={(x)}$로 주어지게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ex: 군집화(clustering), 밀도추정(density estimation), 차원축소(dimension reduction)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cf. semi-supervised learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data partitioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;데이터를 training data와 test data로 나누는 것&lt;ul&gt;
&lt;li&gt;Training data: 모델 학습용&lt;/li&gt;
&lt;li&gt;Test data: 모델 성능 측정용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;왜? Overfitting 방지, 일반화 성능 향상&lt;/li&gt;
&lt;li&gt;보통은 60:40 정도로 데이터를 분할하지만, 보유하고 있는 데이터 규모에 따라 이 비율은 달라지기도 함&lt;/li&gt;
&lt;li&gt;컴퓨터가 모델을 학습하고 평가 받는 것은, 교실에서 학생과 선생님 사이에서 발생하는 일과 매우 유사하다!&lt;ul&gt;
&lt;li&gt;Training phase: 교사는 문제($X_{train}$)와 정답($y_{train}$)이 모두 포함된 training data를 이용해 컴퓨터를 훈련(training)시키고, 컴퓨터는 모델을 학습한다(learning).&lt;/li&gt;
&lt;li&gt;Testing phase: 컴퓨터가 training data로 모델을 학습한 후에는 교사가 컴퓨터에게 정답($y_{test}$) 없이 문제($X_{test}$)만 포함된 시험(test data)을 전달한다. 이 때, 컴퓨터가 제출한 답안지($\hat{y}$)와 실제 정답($y_{test}$)간의 차이를 비교해서 오답/오류(error)가 얼마나 발생했는지 확인함으로써 모델의 성능/성적을 평가한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;때로는 training data, validation data, test data 등 세 개의 그룹으로 데이터를 나누기도 함 (참고: &lt;a href="http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"&gt;What is the difference between test set and validation set?&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/partition.png" width="700px"&gt;&lt;/p&gt;
&lt;h2&gt;Simple linear regression (SLR)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Regression&lt;ul&gt;
&lt;li&gt;대표적인 교사학습(supervised learning) 방법론&lt;ul&gt;
&lt;li&gt;"right answers" given&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Predict continuous valued output&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Formulation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$: 독립변수(independent variable)&lt;/li&gt;
&lt;li&gt;$y$: 종속변수(dependent variable)&lt;/li&gt;
&lt;li&gt;$a, b$: 파라미터(parameters) or 계수(coefficients)&lt;/li&gt;
&lt;li&gt;$\epsilon$: Observation noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y = ax + b + \epsilon$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ex: Housing price prediction&lt;br&gt;
    &lt;img src="images/house.png" width="400px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Multiple linear regression (MLR)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Formulation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_j$: 독립변수들(independent variables)&lt;/li&gt;
&lt;li&gt;$y$: 종속변수(dependent variable)&lt;/li&gt;
&lt;li&gt;$b_j$: 파라미터(parameters) or 계수(coefficients)&lt;/li&gt;
&lt;li&gt;$\epsilon$: Observation noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_mx_m + \epsilon$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;[Programming] SLR, MLR with scikit-learn&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;데이터 import 하기&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;먼저 파이썬에서 MLR을 시행하기 위해서 &lt;a href="http://scikit-learn.org/"&gt;scikit-learn&lt;/a&gt; 패키지를 사용해보자.&lt;/li&gt;
&lt;li&gt;보통은 데이터를 어디선가 다운로드 받고, 정제한 후 읽어들어야겠지만, scikit-learn 패키지에 이미 몇 가지 데이터셋이 준비되어 있으니 그 중 한 가지인 diabetes(당뇨병) 데이터셋을 써보자.&lt;/li&gt;
&lt;li&gt;파이썬에서 패키지를 사용하기 위해서는 &lt;code&gt;import some_package&lt;/code&gt;을 입력하면 되고, 하나의 큰 패키지에서 일부만을 사용할 때는 &lt;code&gt;from some_package import a_subpackage&lt;/code&gt;를 입력하면 된다.
우리는 먼저 scikit-learn의 일부인 &lt;code&gt;dataset&lt;/code&gt; subpackage 사용할 것이니 &lt;code&gt;from sklearn import datasets&lt;/code&gt;를 하고, 데이터를 로딩해보자.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;어렵지 않다. 말 그대로 다른 패키지에서 특정 기능, 혹은 모든 기능을 수입(import)해오겠다는 것이다.&lt;br&gt;(참고: &lt;code&gt;import&lt;/code&gt;에 대해 더 자세히 알기 위해서는 &lt;a href="https://docs.python.org/3/tutorial/modules.html"&gt;모듈에 대한 파이썬 공식 문서&lt;/a&gt;를 보자.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_diabetes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;우리가 다른 곳에서 (ex: &lt;a href="http://archive.ics.uci.edu/ml/"&gt;UCI Datasets&lt;/a&gt;) 데이터를 다운로드 받았다면 별도의 전처리 과정을 거쳐야했겠지만 친절하게도 scikit-learn은 데이터를 이미 전처리해서 data (X), target (y)로 나누어 놓았다. 이를 각각 &lt;code&gt;X&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt;에 넣어보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c"&gt;# returns [&amp;#39;target&amp;#39;, &amp;#39;data&amp;#39;]&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;다른 작업을 진행하기 이전에 X, y 데이터에 대한 탐색을 해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="c"&gt;# returns (442, 10)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simple linear regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;먼저 하나의 변수를 정해 simple linear regression부터 해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;][:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;변수를 선택하고 나면 X2, y를 각각 training set, test set으로 나눈다. 현재 데이터의 개수가 442개이므로 test set을 약 10%인 40개로 해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;X2_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X2_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;slr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# The coefficients&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# mean square error&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RSS: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c"&gt;# Explained variance score: 1 is perfect prediction&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Variance score: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple linear regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data partitioning&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;mlr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# The coefficients&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# mean square error&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RSS: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c"&gt;# Explained variance score: 1 is perfect prediction&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Variance score: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Example source: &lt;a href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"&gt;Linear regression example&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;[Tip] How to write a data mining proposal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Choosing a good project topic&lt;ol&gt;
&lt;li&gt;이 문제를 푸는 것이 의미가 있는가 (있다면 어떤 의미가 있는가?)&lt;/li&gt;
&lt;li&gt;데이터를 구할 수 있는가 (어디서 구할 수 있는가? 정제는 되어 있는가?)&lt;/li&gt;
&lt;li&gt;어떤 접근법/방법론을 사용할 것인가? (내가 주어진 시간 안에 할 수 있는가? 원하는 것을 전부 할 수 없다면, 내가 할 수 있는 범위는 어디까지인가?)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>k-NN</title><link href="http://lucypark.kr/courses/2015-dm/knn.html" rel="alternate"></link><updated>2015-03-09T21:55:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-09:courses/2015-dm/knn.html</id><summary type="html">&lt;h2&gt;k-NN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;k-NN?&lt;/li&gt;
&lt;li&gt;k-Nearest Neighbors?&lt;/li&gt;
&lt;li&gt;k개의 가까운 이웃?&lt;/li&gt;
&lt;li&gt;아이디어: 내 주변의 이웃 k개를 봐서 내가 어떤 값을 가질지 투표하자!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png"&gt;&lt;img src="images/knn.png" width="300px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Calculating similarities&lt;/h3&gt;
&lt;p&gt;k-NN에서 가장 중요한 문제: 두 점 사이의 거리가 가까운지 먼지 어떻게 판단할 것인가? (distance를 정의하는 문제)&lt;/p&gt;
&lt;h3&gt;Multiclass classification&lt;/h3&gt;
&lt;p&gt;What if there are not just two classes, but $k$ classes?
1. One-vs-all
2. One-vs-one
    - $k (k − 1) / 2$ binary classifiers for a K-way multiclass problem&lt;/p&gt;
&lt;h3&gt;One class classification&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;[Programming] k-NN with Scikit-learn&lt;/h2&gt;
&lt;p&gt;코드 출처: http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;데이터 입력하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c"&gt;# take the first two features&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;파라미터 설정하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;k = 15
weights = &amp;#39;uniform&amp;#39;     # uniform, distance 중 택일
algorihtm = &amp;#39;auto&amp;#39;      # ball_tree, kd_tree, brute 중 택일
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;algorithm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.colors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;
&lt;span class="n"&gt;cmap_light&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FFAAAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAFFAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAAAFF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cmap_bold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FF0000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#00FF00&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#0000FF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt; &lt;span class="c"&gt;# step size in the mesh&lt;/span&gt;

&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                     &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

&lt;span class="c"&gt;# Put the result into a color plot&lt;/span&gt;
&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_light&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Plot also the training points&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_bold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;[Programming] Handling real datasets&lt;/h2&gt;
&lt;p&gt;다시 한 번 얘기하지만, 실제 데이터는 이렇게 구조화되어 있지 않다. 보통은 우리가 데이터를 구하고, 전처리해서 지금 로딩하는 데이터의 형태, 또는 우리가 데이터 마이닝 알고리즘을 편하게 돌릴 수 있는 형태로 만들어야 할 것이다.
(그리고 보통 실제 데이터는 아주아주아주 지저분하기 때문에 그렇게 데이터를 전처리하는 시간이 전체 데이터마이닝 프로세스의 절반 이상을 차지하기도 한다.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;예시: &lt;a href="http://land.naver.com/article/articleList.nhn?rletTypeCd=A01&amp;amp;tradeTypeCd=&amp;amp;hscpTypeCd=&amp;amp;cortarNo=1162010200&amp;amp;mapLevel=10"&gt;관악구 신림동 지역의 부동산 가격&lt;/a&gt; 예측하기&lt;/li&gt;
&lt;li&gt;&lt;a href="../tips/terminal.html"&gt;들어가기 전 몇 가지 꿀팁&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data exploration&lt;/h3&gt;
&lt;p&gt;&lt;table class="table"&gt;&lt;tr&gt;&lt;th&gt;공급면적($m^2$)&lt;/th&gt;&lt;th&gt;전용면적($m^2$)&lt;/th&gt;&lt;th&gt;층수&lt;/th&gt;&lt;th&gt;전체층수&lt;/th&gt;&lt;th&gt;매물가(만원)&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;113&lt;/td&gt;&lt;td&gt;84&lt;/td&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;13000&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;116&lt;/td&gt;&lt;td&gt;84&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;38800&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;81&lt;/td&gt;&lt;td&gt;59&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;31500&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;113&lt;/td&gt;&lt;td&gt;84&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;38500&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/p&gt;</summary></entry><entry><title>Assignments</title><link href="http://lucypark.kr/courses/2015-dm/assignments.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/assignments.html</id><summary type="html">&lt;p&gt;For assignment guidelines, visit the class &lt;a href="http://eclass.seoultech.ac.kr"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Assignment 0: 자기소개서 쓰기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-03-11 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A4 용지 1페이지 이내로 자기소개서를 작성해봅시다.
형식은 자유이지만 아래의 내용은 꼭 포함시켜주시기 바랍니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;이름, 전화번호, 메일주소, 사진&lt;/li&gt;
&lt;li&gt;성격, 취미, 특기, 동아리 활동 등&lt;/li&gt;
&lt;li&gt;프로그래밍 경력 (사용 가능한 언어, 상중하 수준, 언어를 이용해 진행한 일)&lt;/li&gt;
&lt;li&gt;데이터마이닝 수업을 듣게 된 이유, 수업을 통해 얻고 싶은 것&lt;/li&gt;
&lt;li&gt;졸업 후 계획, 가고 싶은 학교 또는 회사 &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Project proposal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-03-18 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;팀프로젝트에서 진행할 데이터마이닝 아이디어 한 가지를 제안해주세요.
제안서는 A4 용지 1페이지 이내로 작성하고 PDF로 변환하여 올려주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;프로젝트에 대한 자세한 사항은 &lt;a href="http://www.lucypark.kr/courses/2015-dm/course-introduction.html#term-project-40"&gt;이 링크&lt;/a&gt;를 참고해주세요.&lt;/p&gt;
&lt;h2&gt;Assignment 1: Classification&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-04-02 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assignment 1은 수업 시간에 사용한 MNIST 데이터셋을 이용한 학습을 실제로 해보는 것입니다.
아래의 다섯 가지 문항에 대한 답을 문서로 작성하고 PDF로 변환한 후 올려주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;&lt;a href="logistic-regression.html"&gt;수업 시간에 배운 logistic regression&lt;/a&gt; 알고리즘을 학습해보자.
단, 이번에는 1) Binary가 아니라 전체 10개의 범주(class)에 대해 분류해보고, 2) 성능과 학습 시간을 모두 측정해보는 것이 목표이다.&lt;/p&gt;
&lt;p&gt;먼저 scikit-learn을 통해 MNIST dataset을 loading하고, partitioning해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cross_validation&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fetch_mldata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;MNIST original&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_home&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;참고&lt;/strong&gt;: &lt;code&gt;d = datasets.fetch_mldata('MNIST original', data_home='.')&lt;/code&gt;에서 &lt;code&gt;HTTP Error 500: Internal Server Error&lt;/code&gt;를 내면서 죽는다면 mldata.org 사이트의 접속량 폭주 때문일 수 있다. 이 때 해결법은 두 가지가 있다.&lt;br&gt;
1. 접속이 원활해질 때까지 기다렸다가 잠시 후에 다시 시도해보거나&lt;br&gt;
2. &lt;a href="https://gist.github.com/e9t/736b5410c0937091166b"&gt;이 파이썬 파일&lt;/a&gt;을 현재 디렉토리에서 실행해보자.&lt;br&gt;
현재 디렉토리에 &lt;code&gt;mldata&lt;/code&gt;라는 폴더가 생기고, 그 안에 &lt;code&gt;mnist-original.mat&lt;/code&gt;이라는 이름으로 약 54MB 크기의 파일이 저장되었다면 성공이다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;학습시간을 측정하기 위해 다음과 같이 &lt;a href="https://docs.python.org/3/library/time.html"&gt;time&lt;/a&gt; 모듈을 사용해보자.
다만, 코드가 무슨 말인지는 꼭 이해하고 가자!
이해가 안 된다면 이 참에 구글링도 하고 scikit-learn 공식 문서도 뒤져보자.
우리의 목표는 이 숙제를 완성하는 것이 아니라 실제로 데이터를 가지고 놀 수 있는 능력을 키우는 것이다.&lt;/p&gt;
&lt;p&gt;시간이 오래걸릴 것이니 밥을 먹고 와도 좋다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;1: 로지스틱 회귀분석으로 MNIST 데이터를 학습하는데 시간이 얼마나 걸렸는지 기록하자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;학습이 다 됐는가? Training하느라 우리 컴퓨터가 수고 많았다.
매번 그 과정을 거치면 우리 컴퓨터가 너무 힘드니까 모델을 저장해보자.
이 때 모델의 이름은 길더라도 의미있게 짓는 것이 좋다.
(여기서는 random_state라는 파라미터를 입력했으니 그것을 변수명에 넣어보겠다.)
여기서 파일 확장자로 사용한 &lt;code&gt;pkl&lt;/code&gt;은 &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickle&lt;/a&gt; 파일이라는 것을 나타낸다.
pickle은 파이썬에서 오브젝트(object)를 binary로 저장하는 방식 중 하나이다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.externals&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;joblib&lt;/span&gt;
&lt;span class="n"&gt;joblib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;lr_randomstate_1234.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;위 코드를 입력하면 내 컴퓨터에 한 개(혹은 그 이상)의 파일이 생성되는 것을 확인할 수 있다.
(참고자료: &lt;a href="http://scikit-learn.org/stable/modules/model_persistence.html"&gt;Model persistence&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;이렇게 모델을 파일로 저장하고나면 다음부터는 컴퓨터를 재부팅하고도 다시 많은 시간을 투자해서 모델을 학습하지 않고 아래와 같이 모델을 파일로부터 로딩할 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;joblib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lr_randomstate_1234.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;2: 생성된 학습 모델 파일명과 파일의 크기가 몇 KB인지 기록하자.
파일이 여러 개 생성되었다면 각 파일의 이름과 크기를 모두 기록하자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;우리가 LogisticRegression 클래스의 instance를 생성할 때 사용한 파라미터는 아래의 명령어로 찾을 수 있다.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;3: 어떤 값들이 출력되는가? 각 파라미터는 어떤 의미를 가지는지 설명해보자. 잘 모르겠다면 &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;이 문서&lt;/a&gt;를 확인해보자.&lt;/p&gt;
&lt;p&gt;4: 이 파라미터들을 사용했을 때 test set에 대한 정확도(accuracy), 정밀도(precision), 재현율(recall), F-score는 각각 얼마인지 계산해보자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;5: 분류 알고리즘은 logistic regression 뿐 아니라 k-NN, Decision trees, SVM, Neural Networks 등 다양하게 있다. Logistic regression을 제외하고 다른 분류 알고리즘을 적어도 하나 택해서 Logistic regression으로 MNIST dataset에 대한 학습 시간과 정확도를 비교해보자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;로지스틱 회귀분석 모형의 학습 시간이 엄청 길었을 것이다.
그런데 왜 그랬을까?
&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;공식 문서&lt;/a&gt;에도 나와있듯 multi-class, 즉 다범주 문제에 대해서 scikit-learn의 구현체가 multinomial logistic regression을 구현하지 않았기 때문이다.
즉, 우리가 수업 시간에 다룬 &lt;a href="http://localhost:8000/2015-dm/logistic-regression.html#6-1범주-mnist-1-against-all"&gt;1-against-all&lt;/a&gt; 모델을 모든 범주, 그러니까 총 10개의 범주에 대해서 다 시행한 것이다.
따라서 원래 모델 하나를 학습하는 시간보다 약 10배의 시간이 걸렸을 것이다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Extra credit: 학습 속도를 높이면서 성능은 유지하거나, 학습 속도를 유지하면서 성능을 높일 수 있는가? 어떻게 하면 되는가? 이 때 어떤 파라미터를 썼는가?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Assignment 2: Dimensionality reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-05-07 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Assignment 2은 Assignment 1에서 다룬 MNIST 데이터셋을 이용하여 PCA를 이용한 차원축소를 하는 것입니다.
아래의 세 가지 문항에 대한 답을 문서로 작성하고 PDF로 변환한 후 올려주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;이번에는 Assignment 1에서처럼 &lt;code&gt;sklearn.datasets.fetch_mldata()&lt;/code&gt;를 사용하지 말고,
&lt;code&gt;sklearn.datasets.load_digits()&lt;/code&gt;를 사용해서 약식의 MNIST 데이터를 loading해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_digits&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;      &lt;span class="c"&gt;# sklearn의 datasets에 완비된 mnist 데이터 로딩&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c"&gt;# X, y 변수를 생성하여 각각 독립변수와 종속변수를 넣음&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;1: 변수 X는 몇 개의 record(row)와 몇 개의 attribute(column)로 구성되어 있는가? 첫 다섯 개의 record가 각각 어떤 X값을 가지는지 기록해보자.&lt;/p&gt;
&lt;p&gt;2: 파이썬의 scikit-learn 패키지에서 PCA를 할 수 있는 함수를 찾아 X를 2개의 차원으로 전사하여 Z라는 변수에 저장하자. Z는 몇 개의 record(row)와 몇 개의 attribute(column)로 구성되어 있는가? 또, 첫 다섯 개의 record는 각각 어떤 X값을 가지게 되었는가?&lt;/p&gt;
&lt;p&gt;3: 1, 2번을 수행한 코드를 붙여넣고, 첫째줄부터 마지막줄까지, 작성한 코드의 각 줄이 어떤 동작을 하는지 설명해보자.&lt;/p&gt;
&lt;p&gt;Extra credit: PCA를 이용하여 MNIST 데이터를 두 개의 차원으로 전사한 결과값 Z를 2차원 평면의 그래프로 그려보자. 파이썬을 이용해도 좋고, 전사한 데이터를 파일로 저장하여 다른 언어를 이용해 시각화해도 좋다.
참고로, PCA를 이용하여 &lt;a href="http://en.wikipedia.org/wiki/Iris_flower_data_set"&gt;iris 데이터셋&lt;/a&gt;을 3차원으로 전사시켜 시각화한 예시는 &lt;a href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html"&gt;이 링크&lt;/a&gt;에서 볼 수 있다.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!--
&gt; 5: 분류문제의 성능을 측정할 때는 위의 정확도(accuracy) 뿐 아니라 precision, recall, f-measure 등도 사용된다. 1) 각각은 어떤 의미를 가지며 어떻게 계산하는가? 2) 실제로 값도 구해보고, 3) 도출된 네 가지 지표 accuracy, precision, recall, f-measure을 통해 우리 모델이 잘 학습되었는지 판단해보자.
1. 아래의 코드로 logistic regression의 coefficient들을 그림으로 그려보자.
어떤 모양을 가지는가? 이것이 뜻하는 바는 무엇일까?

        :::python
        plt.matshow(lr.coef_.reshape(28, 28))
        plt.colorbar()
        plt.savefig('lr_coef.png')

## Assignment 3: 분류기 비교

- 마감일: 2015-04-02 23:59
- 제출처: e-class

지금까지 우리는 logistic regression, decision trees, k-NN, ANN, SVM 등의 분류 알고리즘에 대해 배웠거나 배울 것이다.
이들의 특성은 어떻게 다른가?
각각의 알고리즘은 어떤 데이터셋과 상황에 적합하다고 볼 수 있는가?
[iris](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) 데이터셋을 이용해 각 분류기의 성능을 비교해보자.
    - 참고: [Scikit-learn classifier comparison](http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html)
--&gt;</summary><category term="assignments"></category></entry><entry><title>Course Introduction</title><link href="http://lucypark.kr/courses/2015-dm/course-introduction.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/course-introduction.html</id><summary type="html">&lt;h2&gt;About the instructor&lt;/h2&gt;
&lt;p&gt;&lt;img src="http://lucypark.kr/courses/images/me.jpg" width="150px" class="pull-right"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eunjeong (Lucy) Park&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dm.snu.ac.kr/~epark"&gt;PhDc for data mining&lt;/a&gt; at Seoul National University&lt;/li&gt;
&lt;li&gt;a.k.a., &lt;a href="http://lucypark.kr"&gt;lucypark&lt;/a&gt;, &lt;a href="http://twitter.com/echojuliett"&gt;echojuliett&lt;/a&gt;, &lt;a href="http://github.com/e9t"&gt;e9t&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Course logistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Course objective: &lt;strong&gt;Understanding data mining algorithms&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Course website&lt;ul&gt;
&lt;li&gt;Schedule/Lecture notes: &lt;a href="http://lucypark.kr/courses/2015-dm"&gt;http://lucypark.kr/courses/2015-dm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Everything else: &lt;a href="http://eclass.seoultech.ac.kr/"&gt;e-class&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Office hours&lt;ul&gt;
&lt;li&gt;Right after every class&lt;/li&gt;
&lt;li&gt;You are welcome to ask any kind of questions&lt;/li&gt;
&lt;li&gt;You are also encouraged to &lt;a href="mailto:2015-dm@dm.snu.ac.kr"&gt;book ahead&lt;/a&gt;, or your meeting may have to be deferred to another time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Grading&lt;ul&gt;
&lt;li&gt;Assignments (30%): Three graded assignments for you to submit online&lt;/li&gt;
&lt;li&gt;Final exam (30%): In-class exam covering the whole semester&lt;/li&gt;
&lt;li&gt;Term project (40%): Group work solving a real world problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Assignments (30%)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Three graded take-home assignments for you to submit online&lt;/li&gt;
&lt;li&gt;Instead of taking a mid-term exam, we will have assignments for reviewing purposes&lt;/li&gt;
&lt;li&gt;Assignments consist of a quiz and a programming task&lt;ul&gt;
&lt;li&gt;Frankly, the quiz is not for assessment but for you to review and/or preview class materials&lt;/li&gt;
&lt;li&gt;Programming tasks will cover what you have studied during class. You will have done most of the work in class already. What you're going to do at home is to wrap up your work and document it.&lt;/li&gt;
&lt;li&gt;Will be open early, and can be submitted at any time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Note the submission date&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;The assignments will still be open even after the due date but you won't get any credit for solving it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Final exam (30%)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In-class exam covering the whole semester&lt;/li&gt;
&lt;li&gt;Consists of an easy 80%, a relatively hard 20%&lt;ul&gt;
&lt;li&gt;If you have fully understood the contents of the assignments, the easy 80% wouldn't be a problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Term Project (40%)&lt;/h3&gt;
&lt;h4&gt;Proposal (10/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Individual work&lt;/li&gt;
&lt;li&gt;Submit 1 data mining project idea within 1 page (Due: 2015-03-18 23:59)&lt;ul&gt;
&lt;li&gt;Delay penalty: 100% off after due (No delays allowed)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggested contents&lt;ol&gt;
&lt;li&gt;Background: What question do you have? Why is this problem important?&lt;/li&gt;
&lt;li&gt;Formulation: Convert your problem into a data mining problem. What are the inputs and outputs? What algorithms are you going to use? (ex: Classification, clustering, regression, text mining, etc.)&lt;/li&gt;
&lt;li&gt;Data acquisition: How are you going to obtain the data?&lt;/li&gt;
&lt;li&gt;Expected results&lt;/li&gt;
&lt;li&gt;Expected (business) implications&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Evaluation will be based on following criteria&lt;ol&gt;
&lt;li&gt;아이디어가 가지는 의미 (5점)&lt;/li&gt;
&lt;li&gt;아이디어의 구체화 정도 (5점)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Fix the team with a team leader at e-class (Due: 2015-03-18 23:59)&lt;/li&gt;
&lt;li&gt;Choose a topic (Due: 2015-03-29 23:59)&lt;ul&gt;
&lt;li&gt;Among your teammates' proposals, select one topic and conduct a project as a group&lt;/li&gt;
&lt;li&gt;You can also choose topics among &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Progress presentation (15/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Submit presentation slides (Due: 2015-05-07 23:59)&lt;ul&gt;
&lt;li&gt;It is recommended that you have done approx. 70-80% of your whole project by this date&lt;/li&gt;
&lt;li&gt;Submit presentation slides to the e-class&lt;/li&gt;
&lt;li&gt;No page limits&lt;/li&gt;
&lt;li&gt;Delay penalty: 50% off (next day) 100% off (after presentation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggested contents: Your proposal + the following&lt;ol&gt;
&lt;li&gt;(Optional) If you have changed the subject, what was the reason?&lt;/li&gt;
&lt;li&gt;Data Exploration&lt;/li&gt;
&lt;li&gt;What approach you chose to alleviate such questions&lt;/li&gt;
&lt;li&gt;What results you achieved&lt;/li&gt;
&lt;li&gt;What questions you further got and what you plan to do next&lt;/li&gt;
&lt;li&gt;Tricks and tips you want to share with the class&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Presentation day (2015-05-08)&lt;ul&gt;
&lt;li&gt;You will present your project progress in front of the class (Max. 10 min)&lt;/li&gt;
&lt;li&gt;Peer assessment&lt;ul&gt;
&lt;li&gt;You will also be grading your peers' work on presentation day&lt;/li&gt;
&lt;li&gt;You will be given three votes&lt;/li&gt;
&lt;li&gt;You can give one vote to three teams, or give all votes to one team&lt;/li&gt;
&lt;li&gt;You will also be reviewing contributions of your own teammates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Be brief yet clear&lt;/li&gt;
&lt;li&gt;Record the feedback&lt;ul&gt;
&lt;li&gt;One can make the slides&lt;/li&gt;
&lt;li&gt;Another can prepare for the presentation&lt;/li&gt;
&lt;li&gt;And another can do the presentation&lt;/li&gt;
&lt;li&gt;Yet another can take feedback notes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Final report (15/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Present what your group has done throughout the whole project in more than 10 pages (Due: 2015-06-14 23:59)&lt;ul&gt;
&lt;li&gt;Delay penalty: 50% off (next day) 100% off (after that)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You will also be grading your teammates, based on their contributions&lt;/li&gt;
&lt;li&gt;Extra credit will be given to those who submit and/or rank in an open tournament (ex: &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Suggested contents: Your progress report + the following&lt;ul&gt;
&lt;li&gt;Enhanced results&lt;/li&gt;
&lt;li&gt;(Business) implications&lt;/li&gt;
&lt;li&gt;Future work&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;More advice on your projects&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;The best topics are the topics you are actually interested in&lt;ul&gt;
&lt;li&gt;You should be able to "&lt;a href="http://en.wikipedia.org/wiki/Eating_your_own_dog_food"&gt;dogfood&lt;/a&gt;" your own analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Don't be afraid to shift the project's direction&lt;ul&gt;
&lt;li&gt;However, shifting too much will give you less time for real work -- balance!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Feel free to use project results in your graduation project or paper&lt;ul&gt;
&lt;li&gt;Grab two rabbits at once!&lt;/li&gt;
&lt;li&gt;These projects have potential to become something in your portfolio&lt;/li&gt;
&lt;li&gt;May be a plus when you get a job, or apply for grad school&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Asking questions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Never hesitate in asking questions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Private questions: &lt;a href="mailto:2015-dm@dm.snu.ac.kr"&gt;2015-dm@dm.snu.ac.kr&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Personal questions and/or requests&lt;/li&gt;
&lt;li&gt;Assignment submissions that regard privacy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Public questions: Everything else you want to ask goes to e-class&lt;ul&gt;
&lt;li&gt;Using any language of your choice (ex: English, Korean, Java, ...)&lt;/li&gt;
&lt;li&gt;Asking good questions&lt;ul&gt;
&lt;li&gt;Provide as much details as you can&lt;/li&gt;
&lt;li&gt;However, be "brief" and "clear"&lt;/li&gt;
&lt;li&gt;In case of programming questions, explicitly list versions of software being used (including packages and OSs)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="lectures"></category><category term="intro"></category></entry><entry><title>데이터마이닝을 소개합니다</title><link href="http://lucypark.kr/courses/2015-dm/data-mining.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/data-mining.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;이번 시간이 끝나고 나면:
데이터마이닝에 대해 정의할 수 있게 됩니다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;한 마디로 "데이터마이닝"은?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;대용량의 데이터에 담긴 의미있는 규칙을 찾는 일&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;그런데 &lt;a href="http://en.wikipedia.org/wiki/Data_mining"&gt;데이터마이닝(data mining)&lt;/a&gt;은...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;많은 자료 속에 숨어있는 일정한 패턴(규칙)을 발견하는 일이기에 &lt;a href="http://en.wikipedia.org/wiki/Pattern_recognition"&gt;패턴인식(pattern recognition)&lt;/a&gt;의 영역과 맞닿아 있으며&lt;/li&gt;
&lt;li&gt;컴퓨터를 학습(훈련)시키는 &lt;a href="http://en.wikipedia.org/wiki/Machine_learning"&gt;기계학습(machine learning)&lt;/a&gt;과도 유사합니다&lt;/li&gt;
&lt;li&gt;좀더 발전적인 개념으로는 &lt;a href="http://en.wikipedia.org/wiki/Artificial_intelligence"&gt;인공지능(artificial Intelligence)&lt;/a&gt;도 있어요&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="center"&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=data+mining,+pattern+recognition,+machine+learning,+artificial+intelligence&amp;date=1/2014+12m&amp;cmpt=q&amp;tz&amp;tz&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=680&amp;h=330"&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;div class="caption"&gt;2014년 기준 데이터마이닝, 패턴인식, 기계학습, 인공지능에 대한 구글 트렌드. 음? 그렇다면 &lt;a href="http://www.google.com/trends/explore#q=data%20mining%2C%20pattern%20recognition%2C%20machine%20learning%2C%20artificial%20intelligence%2C%20big%20data&amp;cmpt=q&amp;tz="&gt;빅데이터(big data)&lt;/a&gt;는? 데이터사이언스(data science)는?&lt;/div&gt;

&lt;p&gt;이 영역들은 각기 다른 탄생 배경을 가지고, 엄밀하게는 철학과 목적이 상당히 다르기도 하지만, 방법론의 측면에서는 상당히 유사해서 각 영역끼리 서로 배우는 점도 많지요.
사실 공부도 같은 책으로 많이 해요.
(심도있는 공부를 원하시는 분들은 아래 책들도 한 번 찾아보세요!)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ecx.images-amazon.com/images/I/612j5Uo43eL._AA160_.jpg"&gt;&lt;img src="images/book1.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/41LeU3HcBdL._AA160_.jpg"&gt;&lt;img src="images/book2.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/419Ml9MDMaL._AA160_.jpg"&gt;&lt;img src="images/book3.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/51MucLjt9IL._AA160_.jpg"&gt;&lt;img src="images/book4.jpg" height="200px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
iframe {
    width: 700px;
    height: 330px;
}
&lt;/style&gt;

&lt;h2&gt;데이터가 우리 삶을 돕는 다섯 가지 방법&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://readme.skplanet.com/?p=8870"&gt;미래 교통정보 예측: SK Planet T map&lt;/a&gt;&lt;br&gt;
    &lt;a href="http://i1.daumcdn.net/thumb/R750x0/?fname=http%3A%2F%2Fcfile26.uf.tistory.com%2Fimage%2F2204B946524405E408A53F"&gt;&lt;img src="images/tmap.png" width="500px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://watcha.net"&gt;영화 추천: Frograms Watcha&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/watcha.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://google.com"&gt;문서 랭킹: Google&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/google.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://labs.naver.com/tech.html#multimedia_recognition"&gt;음악 인식: Naver 앱 음악 및 와인라벨 인식&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/naver.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.apple.com/ios/siri/"&gt;질의응답 (QA): Apple Siri&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/siri0.jpg" width="30%"&gt;
    &lt;img src="images/siri1.jpg" width="30%"&gt;
    &lt;img src="images/siri2.jpg" width="30%"&gt;&lt;/p&gt;
&lt;p&gt;cf. IBM Watson&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;데이터가 우리 삶을 바꿀 방법&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Jarvis and Samantha&lt;br&gt;
    &lt;img src="images/jarvis.png" width="40%"&gt;
    &lt;img src="images/samantha.png" width="40%"&gt;
    &lt;!--
    &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/ZwOxM0-byvc" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
    --&gt;&lt;/li&gt;
&lt;li&gt;IOT&lt;br&gt;
    &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/NjYTzvAVozo?list=PLl-15sUN2G4eEY2VOqxMEazASNrlMF5FP" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;데이터마이너가 되면 좋은 점&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;세상의 다양한 면을, 다양한 관점에서 살펴볼 수 있습니다&lt;ul&gt;
&lt;li&gt;마케팅부터 시작해서,&lt;/li&gt;
&lt;li&gt;주가의 흐름을 예측하거나(금융),&lt;/li&gt;
&lt;li&gt;DNA 분석이나 MRI 영상을 분석하기도 하며(의료),&lt;/li&gt;
&lt;li&gt;디지털 카메라에서 얼굴 인식(기계)을 하기도 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;사실상 직업이 매일 바뀌는 것이나 다름없음&lt;/li&gt;
&lt;li&gt;물론 그 외에도 우리가 상상할 수 있는 대부분의 영역에 데이터마이닝이 적용된다는 사실!&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;다음 시간 예고:
데이터마이닝을 할 때 가장 중요한 것은? Asking the right question.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="seoultech"></category><category term="lectures"></category></entry><entry><title>Data Mining (Spring 2015)</title><link href="http://lucypark.kr/courses/2015-dm/index.html" rel="alternate"></link><updated>2015-03-02T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-02:courses/2015-dm/index.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Welcome to Data Mining class of 2015!
Here you'll find all course materials, guides and schedules.
For discussions, please visit the class &lt;a href="http://eclass.seoultech.ac.kr/"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;데이터마이닝(data mining)이란 대용량 데이터베이스에 존재하는 데이터에서 관계, 패턴, 규칙 등을 찾아내고 모형화해서 의사결정을 돕는 유용한 정보로 변환하는 일련의 과정이다.
본 강좌에서는 기술 모델링(descriptive modeling)과 예측 모델링(predictive modeling)에 사용되는 탐색적 통계, 기계학습, 범주형 자료분석 기법들을 공부하고 응용사례 연구와 패키지를 이용한 프로젝트를 수행한다.&lt;/p&gt;
&lt;h3&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;한 가지 이상의 프로그래밍 언어에 대한 친숙함 (ex: R, Python, Java, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What you will learn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;데이터마이닝의 기본 개념, 구축 프로세스 학습 및 비즈니스 활용 사례 인식&lt;/li&gt;
&lt;li&gt;데이터마이닝의 주요 방법론의 이론적 토대 학습 및 응용 분야 학습&lt;/li&gt;
&lt;li&gt;데이터마이닝 분야에서 널리 사용되는 오픈소스 데이터 분석 언어인 &lt;a href="https://python.org/"&gt;파이썬&lt;/a&gt; 사용 방법 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Grading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Assignments (30%)&lt;/li&gt;
&lt;li&gt;Final exam (30%)&lt;/li&gt;
&lt;li&gt;Term Project (40%)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Schedule&lt;/h2&gt;
&lt;p&gt;For assignment guidelines, visit the class &lt;a href="http://eclass.seoultech.ac.kr"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;table id="schedule" class="table table-bordered"&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;date&lt;/th&gt;&lt;th&gt;lecture&lt;/th&gt;&lt;/tr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/06&lt;/td&gt;&lt;td&gt;&lt;a href="course-introduction.html"&gt;Course introduction&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="data-mining.html"&gt;Data mining&lt;/a&gt;&lt;li&gt;Assignment 0 (Due: 3/11) + Project proposal (Due: 3/18)&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/11&lt;/td&gt;&lt;td&gt;Due date: Assignment 0&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/13&lt;/td&gt;&lt;td&gt;&lt;a href="multiple-linear-regression.html"&gt;지도학습 1: Multiple linear regression&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="../tips/introduction-to-python.html"&gt;Introduction to Python&lt;/a&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/18&lt;/td&gt;&lt;td&gt;Due date: Project proposal, Fix project teams&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/20&lt;/td&gt;&lt;td&gt;&lt;a href="logistic-regression.html"&gt;지도학습 2: Logistic regression&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="assignments.html#assignment-1-classification"&gt;Assignment 1 (Due: 4/2)&lt;/a&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/27&lt;/td&gt;&lt;td&gt;&lt;a href="http://eclass.seoultech.ac.kr/ilos/co/efile_download.acl?FILE_SEQ=84201&amp;CONTENT_SEQ=89718"&gt;지도학습 3: Decision tree + k-NN&lt;/a&gt;&lt;ul&gt;&lt;li&gt;In class project discussions&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/29&lt;/td&gt;&lt;td&gt;Due date: Choose team project topic&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;4/02&lt;/td&gt;&lt;td&gt;Due date: Assignment 1&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/03&lt;/td&gt;&lt;td&gt;&lt;a href="http://eclass.seoultech.ac.kr/ilos/co/efile_download.acl?FILE_SEQ=88162&amp;CONTENT_SEQ=93265"&gt;텍스트 마이닝 1: 이론&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/10&lt;/td&gt;&lt;td&gt;&lt;a href="text-mining.html"&gt;텍스트 마이닝 2: 실습&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/17&lt;/td&gt;&lt;td&gt;변수 선택과 차원 축소&lt;ul&gt;&lt;li&gt;&lt;a href="assignments.html#assignment-2-dimensionality-reduction"&gt;Assignment 2 (Due: 5/07)&lt;/a&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/24&lt;/td&gt;&lt;td&gt;비지도학습 1: k-means + hierarchical clustering&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/01&lt;/td&gt;&lt;td&gt;비지도학습 2: Market basket analysis&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;5/07&lt;/td&gt;&lt;td&gt;Due date: Assignment 2&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;5/07&lt;/td&gt;&lt;td&gt;Due date: Team project presentation slides&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/08&lt;/td&gt;&lt;td&gt;Team project presentations&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/15&lt;/td&gt;&lt;td&gt;지도학습 5: Artificial neural networks&lt;ul&gt;&lt;li&gt;Assignment 3 (Due: 6/4)&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/22&lt;/td&gt;&lt;td&gt;지도학습 6: Support vector machines&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/29&lt;/td&gt;&lt;td&gt;데이터 시각화&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;6/04&lt;/td&gt;&lt;td&gt;Due date: Assignment 3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6/05&lt;/td&gt;&lt;td&gt;대용량 데이터마이닝&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6/12&lt;/td&gt;&lt;td&gt;Final Exam&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;6/14&lt;/td&gt;&lt;td&gt;Due date: Team project final report&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;</summary><category term="data"></category><category term="seoultech"></category><category term="lecturer"></category><category term="syllabus"></category></entry></feed>