<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Courses</title><link href="http://lucypark.kr/courses/" rel="alternate"></link><link href="http://lucypark.kr/courses/feeds/2015-dm.atom.xml" rel="self"></link><id>http://lucypark.kr/courses/</id><updated>2015-05-08T09:00:00+09:00</updated><entry><title>Artificial neural networks</title><link href="http://lucypark.kr/courses/2015-dm/artificial-neural-networks.html" rel="alternate"></link><updated>2015-05-08T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-05-08:courses/2015-dm/artificial-neural-networks.html</id><summary type="html">&lt;h2&gt;So, is there a one-size-fits-all algorithm?&lt;/h2&gt;
&lt;p&gt;No!
The best algorithm depends on the characteristics of the trainin data.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;Logistic Regression&lt;/td&gt;&lt;td&gt;Decision Tree&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Does it require variables to be normally distributed?&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Does it suffer multicollinearity issue? &lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;No&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Does it do as well with categorical variables?&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;td&gt;Yes&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Does it conduct variable selection without stepwise?&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Does it apply to sparse data?&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;http://www.quora.com/What-are-the-advantages-of-different-classification-algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;"There's no such thing as a free lunch"&lt;/h2&gt;
&lt;h3&gt;Bias-variance tradeoff&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bias-variance decomposition
http://scott.fortmann-roe.com/docs/BiasVariance.html&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Time-performance tradeoff&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;TBD&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Dimensionality reduction</title><link href="http://lucypark.kr/courses/2015-dm/dimensionality-reduction.html" rel="alternate"></link><updated>2015-03-17T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-17:courses/2015-dm/dimensionality-reduction.html</id><summary type="html">&lt;h2&gt;Variable selection in regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why should we select a subset of variables?&lt;ul&gt;
&lt;li&gt;May be expensive or not feasible to collect a full complement of predictors for future prediction.&lt;/li&gt;
&lt;li&gt;May be able to measure fewer predictors more accurately (e.g. in surveys)&lt;/li&gt;
&lt;li&gt;More predictors, more missing values&lt;/li&gt;
&lt;li&gt;Parsimony (a.k.a. Occam’s Razor): the simpler, the better.&lt;/li&gt;
&lt;li&gt;Multicollinearity: presence of two or more predictors sharing the same linear relationship with the outcome variables&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Goal: 일반화 성능 향상&lt;ul&gt;
&lt;li&gt;More robust&lt;/li&gt;
&lt;li&gt;Higher predictive accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exhaustive search: 모든 변수 조합의 성능을 살펴봄&lt;ul&gt;
&lt;li&gt;${x_1}, {x_2}, {x_3}, ..., {x_1, x_2}, {x_2, x_3}, ..., {x_1, x_2, x_3}, ...$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial search&lt;ul&gt;
&lt;li&gt;Forward selection: 유의성이 큰 변수는 하나씩 더함&lt;ul&gt;
&lt;li&gt;${x_4} \to {x_4, x_5} \to {x_1, x_4, x_5} \to {x_1, x_3, x_4, x_5} \to {x_1, x_2, x_3, x_4, x_5}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backward elimination: 유의성이 작은 변수를 하나씩 제거&lt;ul&gt;
&lt;li&gt;${x_1, x_2, x_3, x_4, x_5} \to {x_1, x_3, x_4, x_5} \to {x_1, x_4, x_5} \to {x_4, x_5} to{x_4}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stepwise selection: 유의성이 큰 변수를 더하거나 작은 변수를 하나씩 제거&lt;ul&gt;
&lt;li&gt;${x_4} \to {x_4, x_5} \to {x_1, x_4, x_5} \to {x_1, x_5} \to ...$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Recommendation systems</title><link href="http://lucypark.kr/courses/2015-dm/recommendation-systems.html" rel="alternate"></link><updated>2015-03-16T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-16:courses/2015-dm/recommendation-systems.html</id><summary type="html">&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;http://www.slideshare.net/kyuhwanjung/cvprml-20140111&lt;/li&gt;
&lt;li&gt;http://www.slideshare.net/xlos21/deview-recopick-aws?related=1&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Multiple linear regression</title><link href="http://lucypark.kr/courses/2015-dm/multiple-linear-regression.html" rel="alternate"></link><updated>2015-03-16T18:37:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-13:courses/2015-dm/multiple-linear-regression.html</id><summary type="html">&lt;h2&gt;오늘의 목표&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;데이터마이닝 알고리즘의 분류&lt;/li&gt;
&lt;li&gt;Data partitioning 개념 익히기&lt;/li&gt;
&lt;li&gt;그 외 데이터마이닝에서 빈번히 다루는 용어들 익히기&lt;/li&gt;
&lt;li&gt;단변수, 다변수 선형회귀분석(linear regression)의 개념 익히고 파이썬으로 돌려보기&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data mining process&lt;/h2&gt;
&lt;p&gt;&lt;img src="images/process.png" width="500px"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Problem definition&lt;/li&gt;
&lt;li&gt;Data acquisition and selection&lt;/li&gt;
&lt;li&gt;Data exploration&lt;/li&gt;
&lt;li&gt;Data preprocessing&lt;/li&gt;
&lt;li&gt;Train and evaluate data mining model&lt;/li&gt;
&lt;li&gt;Interpret results&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;목적에 따른 분류: Predictive methods vs Descriptive methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Predictive modeling&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Predict the &lt;em&gt;future&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Identify strong links between variables of data&lt;/li&gt;
&lt;li&gt;To predict the unknown consequence (dependent variable) based on the information provided (independent variable)&lt;ul&gt;
&lt;li&gt;과거를 통해 현재를 알고 미래를 내다보자&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Descriptive modeling&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Look back to the &lt;em&gt;past&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;To extract compact and easily understood information from large, sometimes gigantic databases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/methods.png" width="600px"&gt;&lt;/p&gt;
&lt;h3&gt;학습데이터에 따른 분류: Supervised learning vs Unsupervised learning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;교사학습(Supervised learning)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Goal: predict a single "target" or "outcome" variable $y$&lt;ul&gt;
&lt;li&gt;Finds relations between $X$ and $y$: $y = f(x_1, x_2, ..., x_n) + \epsilon$&lt;/li&gt;
&lt;li&gt;입출력(input-output)의 쌍으로 구성된 training data로부터 입력을 출력을 사상하는 함수를 학습하는 과정&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Method: &lt;em&gt;Learn&lt;/em&gt; on training data, &lt;em&gt;score&lt;/em&gt; on test data&lt;ul&gt;
&lt;li&gt;즉, 입력벡터를 $X$, 그에 대응하는 출력벡터(i.e., label)를 $y$라고 할 때, training data는 $D={(x, y)}$로 주어지게 되며, 모델은 이 training data에 기반하여 관측하지 않은 새로운 데이터 $x'$가 들어왔을 때 그에 해당되는 label, $y'$을 추론하는 방법을 배우게 된다&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ex: 분류(classification)와 회귀분석(regression)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;비교사학습(Unsupervised learning)&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Goal: Explore intrinsic characteristics of data $X$&lt;/li&gt;
&lt;li&gt;Method: Estimate underlying distributions and/or segment data into meaningful groups or detect patterns&lt;ul&gt;
&lt;li&gt;There is no target (outcome) variable to predict or classify&lt;/li&gt;
&lt;li&gt;출력값 없이 오직 입력값만 주어지며, 이러한 입력값들의 공통적인 특성을 파악하여 학습하는 과정&lt;/li&gt;
&lt;li&gt;Training data는 $D={(x)}$로 주어지게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ex: 군집화(clustering), 밀도추정(density estimation), 차원축소(dimension reduction)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cf. semi-supervised learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data partitioning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;데이터를 training data와 test data로 나누는 것&lt;ul&gt;
&lt;li&gt;Training data: 모델 학습용&lt;/li&gt;
&lt;li&gt;Test data: 모델 성능 측정용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;보통은 60:40 정도로 데이터를 분할하지만, 보유하고 있는 데이터 규모에 따라 이 비율은 달라지기도 함&lt;/li&gt;
&lt;li&gt;컴퓨터가 모델을 학습하고 평가 받는 것은, 교실에서 학생과 선생님 사이에서 발생하는 일과 매우 유사하다!&lt;ul&gt;
&lt;li&gt;Training phase: 교사는 문제($X_{train}$)와 정답($y_{train}$)이 모두 포함된 training data를 이용해 컴퓨터를 훈련(training)시키고, 컴퓨터는 모델을 학습한다(learning).&lt;/li&gt;
&lt;li&gt;Testing phase: 컴퓨터가 training data로 모델을 학습한 후에는 교사가 컴퓨터에게 정답($y_{test}$) 없이 문제($X_{test}$)만 포함된 시험(test data)을 전달한다. 이 때, 컴퓨터가 제출한 답안지($\hat{y}$)와 실제 정답($y_{test}$)간의 차이를 비교해서 오답/오류(error)가 얼마나 발생했는지 확인함으로써 모델의 성능/성적을 평가한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;때로는 training data, validation data, test data 등 세 개의 그룹으로 데이터를 나누기도 함 (참고: &lt;a href="http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"&gt;What is the difference between test set and validation set?&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/partition.png" width="700px"&gt;&lt;/p&gt;
&lt;h2&gt;Simple linear regression (SLR)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Regression&lt;ul&gt;
&lt;li&gt;대표적인 교사학습(supervised learning) 방법론&lt;ul&gt;
&lt;li&gt;"right answers" given&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Predict continuous valued output&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Formulation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x$: 독립변수(independent variable)&lt;/li&gt;
&lt;li&gt;$y$: 종속변수(dependent variable)&lt;/li&gt;
&lt;li&gt;$a, b$: 파라미터(parameters) or 계수(coefficients)&lt;/li&gt;
&lt;li&gt;$\epsilon$: Observation noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y = ax + b + \epsilon$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ex: Housing price prediction&lt;br&gt;
    &lt;img src="images/house.png" width="400px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Multiple linear regression (MLR)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Formulation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_j$: 독립변수들(independent variables)&lt;/li&gt;
&lt;li&gt;$y$: 종속변수(dependent variable)&lt;/li&gt;
&lt;li&gt;$b_j$: 파라미터(parameters) or 계수(coefficients)&lt;/li&gt;
&lt;li&gt;$\epsilon$: Observation noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_mx_m + \epsilon$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;[Programming] SLR, MLR with scikit-learn&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;데이터 import 하기&lt;ul&gt;
&lt;li&gt;먼저 파이썬에서 MLR을 시행하기 위해서 &lt;a href="http://scikit-learn.org/"&gt;scikit-learn&lt;/a&gt; 패키지를 사용해보자.&lt;/li&gt;
&lt;li&gt;보통은 데이터를 어디선가 다운로드 받고, 정제한 후 읽어들어야겠지만, scikit-learn 패키지에 이미 몇 가지 데이터셋이 준비되어 있으니 그 중 한 가지인 diabetes(당뇨병) 데이터셋을 써보자.&lt;/li&gt;
&lt;li&gt;파이썬에서 패키지를 사용하기 위해서는 &lt;code&gt;import some_package&lt;/code&gt;을 입력하면 되고, 하나의 큰 패키지에서 일부만을 사용할 때는 &lt;code&gt;from some_package import a_subpackage&lt;/code&gt;를 입력하면 된다.
우리는 먼저 scikit-learn의 일부인 &lt;code&gt;dataset&lt;/code&gt; subpackage 사용할 것이니 &lt;code&gt;from sklearn import datasets&lt;/code&gt;를 하고, 데이터를 로딩해보자.&lt;/li&gt;
&lt;li&gt;어렵지 않다. 말 그대로 다른 패키지에서 특정 기능, 혹은 모든 기능을 수입(import)해오겠다는 것이다.&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_diabetes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;참고: import에 대해 더 자세히 알기 위해서는 &lt;a href="https://docs.python.org/3/tutorial/modules.html"&gt;모듈에 대한 파이썬 공식 문서&lt;/a&gt;를 보자.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="err"&gt;우리가&lt;/span&gt; &lt;span class="err"&gt;다른&lt;/span&gt; &lt;span class="err"&gt;곳에서&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nl"&gt;ex&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;UCI&lt;/span&gt; &lt;span class="n"&gt;Datasets&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="nl"&gt;http&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//archive.ics.uci.edu/ml/)) 데이터를 다운로드 받았다면 별도의 전처리 과정을 거쳐야했겠지만 친절하게도 scikit-learn은 데이터를 이미 전처리해서 data (X), target (y)로 나누어 놓았다. 이를 각각 `X`, `y`에 넣어보자.&lt;/span&gt;

        &lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;
        &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="err"&gt;다른&lt;/span&gt; &lt;span class="err"&gt;작업을&lt;/span&gt; &lt;span class="err"&gt;진행하기&lt;/span&gt; &lt;span class="err"&gt;이전에&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="err"&gt;데이터에&lt;/span&gt; &lt;span class="err"&gt;대한&lt;/span&gt; &lt;span class="err"&gt;탐색을&lt;/span&gt; &lt;span class="err"&gt;해보자&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;

        &lt;span class="o"&gt;:::&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;
        &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;      &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;442&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Simple linear regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;먼저 하나의 변수를 정해 simple linear regression부터 해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;X2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;][:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;변수를 선택하고 나면 X2, y를 각각 training set, test set으로 나눈다. 현재 데이터의 개수가 442개이므로 test set을 약 10%인 40개로 해보자.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;X2_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X2_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;slr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# The coefficients&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# mean square error&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RSS: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c"&gt;# Explained variance score: 1 is perfect prediction&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Variance score: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;slr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X2_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="p"&gt;(())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Multiple linear regression&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Data partitioning&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;
&lt;span class="n"&gt;mlr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LinearRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# The coefficients&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Coefficients: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# mean square error&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RSS: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c"&gt;# Explained variance score: 1 is perfect prediction&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Variance score: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;mlr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Example source: &lt;a href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html"&gt;Linear regression example&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;[Tip] How to write a data mining proposal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Choosing a good project topic&lt;ol&gt;
&lt;li&gt;이 문제를 푸는 것이 의미가 있는가 (있다면 어떤 의미가 있는가?)&lt;/li&gt;
&lt;li&gt;데이터를 구할 수 있는가 (어디서 구할 수 있는가? 정제는 되어 있는가?)&lt;/li&gt;
&lt;li&gt;어떤 접근법/방법론을 사용할 것인가? (내가 주어진 시간 안에 할 수 있는가? 원하는 것을 전부 할 수 없다면, 내가 할 수 있는 범위는 어디까지인가?)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>k-NN</title><link href="http://lucypark.kr/courses/2015-dm/knn.html" rel="alternate"></link><updated>2015-03-09T21:55:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-09:courses/2015-dm/knn.html</id><summary type="html">&lt;h2&gt;k-NN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;k-NN?&lt;/li&gt;
&lt;li&gt;k-Nearest Neighbors?&lt;/li&gt;
&lt;li&gt;k개의 가까운 이웃?&lt;/li&gt;
&lt;li&gt;아이디어: 내 주변의 이웃 k개를 봐서 내가 어떤 값을 가질지 투표하자!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png"&gt;&lt;img src="images/knn.png" width="300px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Calculating similarities&lt;/h3&gt;
&lt;p&gt;k-NN에서 가장 중요한 문제: 두 점 사이의 거리가 가까운지 먼지 어떻게 판단할 것인가? (distance를 정의하는 문제)&lt;/p&gt;
&lt;h3&gt;Multiclass classification&lt;/h3&gt;
&lt;p&gt;What if there are not just two classes, but $k$ classes?
1. One-vs-all
2. One-vs-one
    - $k (k − 1) / 2$ binary classifiers for a K-way multiclass problem&lt;/p&gt;
&lt;h3&gt;One class classification&lt;/h3&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;h2&gt;[Programming] k-NN with Scikit-learn&lt;/h2&gt;
&lt;p&gt;코드 출처: http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;데이터 입력하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;   &lt;span class="c"&gt;# take the first two features&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;파라미터 설정하기&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;k = 15
weights = &amp;#39;uniform&amp;#39;     # uniform, distance 중 택일
algorihtm = &amp;#39;auto&amp;#39;      # ball_tree, kd_tree, brute 중 택일
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Train&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;neighbors&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;algorithm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plot&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.colors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;
&lt;span class="n"&gt;cmap_light&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FFAAAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAFFAA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#AAAAFF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;cmap_bold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#FF0000&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#00FF00&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#0000FF&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt; &lt;span class="c"&gt;# step size in the mesh&lt;/span&gt;

&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;meshgrid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                     &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;c_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;ravel&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

&lt;span class="c"&gt;# Put the result into a color plot&lt;/span&gt;
&lt;span class="n"&gt;Z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pcolormesh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_light&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Plot also the training points&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cmap_bold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;xx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;yy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;</summary></entry><entry><title>Assignments</title><link href="http://lucypark.kr/courses/2015-dm/assignments.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/assignments.html</id><summary type="html">&lt;p&gt;For assignment guidelines, visit the class &lt;a href="http://eclass.seoultech.ac.kr"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Assignment 0: 자기소개서 쓰기&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-03-11 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A4 용지 1페이지 이내로 자기소개서를 작성해봅시다.
형식은 자유이지만 아래의 내용은 꼭 포함시켜주시기 바랍니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;이름, 전화번호, 메일주소, 사진&lt;/li&gt;
&lt;li&gt;성격, 취미, 특기, 동아리 활동 등&lt;/li&gt;
&lt;li&gt;프로그래밍 경력 (사용 가능한 언어, 상중하 수준, 언어를 이용해 진행한 일)&lt;/li&gt;
&lt;li&gt;데이터마이닝 수업을 듣게 된 이유, 수업을 통해 얻고 싶은 것&lt;/li&gt;
&lt;li&gt;졸업 후 계획, 가고 싶은 학교 또는 회사 &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Project proposal&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-03-18 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;팀프로젝트에서 진행할 데이터마이닝 아이디어 한 가지를 제안해주세요.
제안서는 A4 용지 1페이지 이내로 작성하고 PDF로 변환하여 올려주시기 바랍니다.&lt;/p&gt;
&lt;p&gt;프로젝트에 대한 자세한 사항은 &lt;a href="http://www.lucypark.kr/courses/2015-dm/course-introduction.html#term-project-40"&gt;이 링크&lt;/a&gt;를 참고해주세요.&lt;/p&gt;
&lt;h2&gt;Assignment 1: 분류기 비교&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;마감일: 2015-06-04 23:59&lt;/li&gt;
&lt;li&gt;제출처: e-class&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;다음의 질문에 답하자.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;지금까지 우리는 logistic regression, decision trees, k-NN 등의 분류 알고리즘에 대해 배웠다.
이 들의 특성은 어떻게 다른가?
각각의 알고리즘은 어떤 데이터셋과 상황에 적합하다고 볼 수 있는가?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;그 외에 자주 사용되는 분류 알고리즘에는 무엇이 있는지 조사해보자.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;참고: http://scikit-learn.org/stable/auto_examples/plot_classifier_comparison.html&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</summary><category term="assignments"></category></entry><entry><title>Course Introduction</title><link href="http://lucypark.kr/courses/2015-dm/course-introduction.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/course-introduction.html</id><summary type="html">&lt;h2&gt;About the instructor&lt;/h2&gt;
&lt;p&gt;&lt;img src="http://lucypark.kr/courses/images/me.jpg" width="150px" class="pull-right"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eunjeong (Lucy) Park&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dm.snu.ac.kr/~epark"&gt;PhDc for data mining&lt;/a&gt; at Seoul National University&lt;/li&gt;
&lt;li&gt;a.k.a., &lt;a href="http://lucypark.kr"&gt;lucypark&lt;/a&gt;, &lt;a href="http://twitter.com/echojuliett"&gt;echojuliett&lt;/a&gt;, &lt;a href="http://github.com/e9t"&gt;e9t&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Course logistics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Course objective: &lt;strong&gt;Understanding data mining algorithms&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Course website&lt;ul&gt;
&lt;li&gt;Schedule/Lecture notes: &lt;a href="http://lucypark.kr/courses/2015-dm"&gt;http://lucypark.kr/courses/2015-dm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Everything else: &lt;a href="http://eclass.seoultech.ac.kr/"&gt;e-class&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Office hours&lt;ul&gt;
&lt;li&gt;Right after every class&lt;/li&gt;
&lt;li&gt;You are welcome to ask any kind of questions&lt;/li&gt;
&lt;li&gt;You are also encouraged to &lt;a href="mailto:2015-dm@dm.snu.ac.kr"&gt;book ahead&lt;/a&gt;, or your meeting may have to be deferred to another time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Grading&lt;ul&gt;
&lt;li&gt;Assignments (30%): Three graded assignments for you to submit online&lt;/li&gt;
&lt;li&gt;Final exam (30%): In-class exam covering the whole semester&lt;/li&gt;
&lt;li&gt;Term project (40%): Group work solving a real world problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Assignments (30%)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Three graded take-home assignments for you to submit online&lt;/li&gt;
&lt;li&gt;Instead of taking a mid-term exam, we will have assignments for reviewing purposes&lt;/li&gt;
&lt;li&gt;Assignments consist of a quiz and a programming task&lt;ul&gt;
&lt;li&gt;Frankly, the quiz is not for assessment but for you to review and/or preview class materials&lt;/li&gt;
&lt;li&gt;Programming tasks will cover what you have studied during class. You will have done most of the work in class already. What you're going to do at home is to wrap up your work and document it.&lt;/li&gt;
&lt;li&gt;Will be open early, and can be submitted at any time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Note the submission date&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;The assignments will still be open even after the due date but you won't get any credit for solving it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Final exam (30%)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In-class exam covering the whole semester&lt;/li&gt;
&lt;li&gt;Consists of an easy 80%, a relatively hard 20%&lt;ul&gt;
&lt;li&gt;If you have fully understood the contents of the assignments, the easy 80% wouldn't be a problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Term Project (40%)&lt;/h3&gt;
&lt;h4&gt;Proposal (10/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Individual work&lt;/li&gt;
&lt;li&gt;Submit 1 data mining project idea within 1 page (Due: 2015-03-18 23:59)&lt;ul&gt;
&lt;li&gt;Delay penalty: 100% off after due (No delays allowed)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggested contents&lt;ol&gt;
&lt;li&gt;Background: What question do you have? Why is this problem important?&lt;/li&gt;
&lt;li&gt;Formulation: Convert your problem into a data mining problem. What are the inputs and outputs? What algorithms are you going to use? (ex: Classification, clustering, regression, text mining, etc.)&lt;/li&gt;
&lt;li&gt;Data acquisition: How are you going to obtain the data?&lt;/li&gt;
&lt;li&gt;Expected results&lt;/li&gt;
&lt;li&gt;Expected (business) implications&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Evaluation will be based on following criteria&lt;ol&gt;
&lt;li&gt;아이디어가 가지는 의미 (5점)&lt;/li&gt;
&lt;li&gt;아이디어의 구체화 정도 (5점)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Fix the team with a team leader at e-class (Due: 2015-03-18 23:59)&lt;/li&gt;
&lt;li&gt;Choose a topic (Due: 2015-03-29 23:59)&lt;ul&gt;
&lt;li&gt;Among your teammates' proposals, select one topic and conduct a project as a group&lt;/li&gt;
&lt;li&gt;You can also choose topics among &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Progress presentation (15/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Submit presentation slides (Due: 2015-04-30 23:59)&lt;ul&gt;
&lt;li&gt;It is recommended that you have done approx. 70-80% of your whole project by this date&lt;/li&gt;
&lt;li&gt;Submit presentation slides to the e-class&lt;/li&gt;
&lt;li&gt;No page limits&lt;/li&gt;
&lt;li&gt;Delay penalty: 50% off (next day) 100% off (after presentation)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Suggested contents: Your proposal + the following&lt;ol&gt;
&lt;li&gt;Data Exploration&lt;/li&gt;
&lt;li&gt;What approach you chose to alleviate such questions&lt;/li&gt;
&lt;li&gt;What results you achieved&lt;/li&gt;
&lt;li&gt;What questions you further got and what you plan to do next&lt;/li&gt;
&lt;li&gt;Tricks and tips you want to share with the class&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Presentation day (2015-05-01)&lt;ul&gt;
&lt;li&gt;You will present your project progress in front of the class (Max. 10 min)&lt;/li&gt;
&lt;li&gt;Peer assessment&lt;ul&gt;
&lt;li&gt;You will also be grading your peers' work on presentation day&lt;/li&gt;
&lt;li&gt;You will be given three votes&lt;/li&gt;
&lt;li&gt;You can give one vote to three teams, or give all votes to one team&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Be brief yet clear&lt;/li&gt;
&lt;li&gt;Record the feedback&lt;ul&gt;
&lt;li&gt;One can make the slides&lt;/li&gt;
&lt;li&gt;Another can prepare for the presentation&lt;/li&gt;
&lt;li&gt;And another can do the presentation&lt;/li&gt;
&lt;li&gt;Yet another can take feedback notes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Final report (15/40)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Present what your group has done throughout the whole project in more than 10 pages (Due: 2015-06-14 23:59)&lt;ul&gt;
&lt;li&gt;Delay penalty: 50% off (next day) 100% off (after that)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You will also be grading your teammates, based on their contributions&lt;/li&gt;
&lt;li&gt;Extra credit will be given to those who submit and/or rank in an open tournament (ex: &lt;a href="http://kaggle.com"&gt;Kaggle&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Suggested contents: Your progress report + the following&lt;ul&gt;
&lt;li&gt;Enhanced results&lt;/li&gt;
&lt;li&gt;(Business) implications&lt;/li&gt;
&lt;li&gt;Future work&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;More advice on your projects&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;The best topics are the topics you are actually interested in&lt;ul&gt;
&lt;li&gt;You should be able to "&lt;a href="http://en.wikipedia.org/wiki/Eating_your_own_dog_food"&gt;dogfood&lt;/a&gt;" your own analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Don't be afraid to shift the project's direction&lt;ul&gt;
&lt;li&gt;However, shifting too much will give you less time for real work -- balance!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Feel free to use project results in your graduation project or paper&lt;ul&gt;
&lt;li&gt;Grab two rabbits at once!&lt;/li&gt;
&lt;li&gt;These projects have potential to become something in your portfolio&lt;/li&gt;
&lt;li&gt;May be a plus when you get a job, or apply for grad school&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Asking questions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Never hesitate in asking questions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Private questions: &lt;a href="mailto:2015-dm@dm.snu.ac.kr"&gt;2015-dm@dm.snu.ac.kr&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Personal questions and/or requests&lt;/li&gt;
&lt;li&gt;Assignment submissions that regard privacy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Public questions: Everything else you want to ask goes to e-class&lt;ul&gt;
&lt;li&gt;Using any language of your choice (ex: English, Korean, Java, ...)&lt;/li&gt;
&lt;li&gt;Asking good questions&lt;ul&gt;
&lt;li&gt;Provide as much details as you can&lt;/li&gt;
&lt;li&gt;However, be "brief" and "clear"&lt;/li&gt;
&lt;li&gt;In case of programming questions, explicitly list versions of software being used (including packages and OSs)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="lectures"></category><category term="intro"></category></entry><entry><title>데이터마이닝을 소개합니다</title><link href="http://lucypark.kr/courses/2015-dm/data-mining.html" rel="alternate"></link><updated>2015-03-06T09:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-06:courses/2015-dm/data-mining.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;이번 시간이 끝나고 나면:
데이터마이닝에 대해 정의할 수 있게 됩니다. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;한 마디로 "데이터마이닝"은?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;대용량의 데이터에 담긴 의미있는 규칙을 찾는 일&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;그런데 &lt;a href="http://en.wikipedia.org/wiki/Data_mining"&gt;데이터마이닝(data mining)&lt;/a&gt;은...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;많은 자료 속에 숨어있는 일정한 패턴(규칙)을 발견하는 일이기에 &lt;a href="http://en.wikipedia.org/wiki/Pattern_recognition"&gt;패턴인식(pattern recognition)&lt;/a&gt;의 영역과 맞닿아 있으며&lt;/li&gt;
&lt;li&gt;컴퓨터를 학습(훈련)시키는 &lt;a href="http://en.wikipedia.org/wiki/Machine_learning"&gt;기계학습(machine learning)&lt;/a&gt;과도 유사합니다&lt;/li&gt;
&lt;li&gt;좀더 발전적인 개념으로는 &lt;a href="http://en.wikipedia.org/wiki/Artificial_intelligence"&gt;인공지능(artificial Intelligence)&lt;/a&gt;도 있어요&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="center"&gt;
&lt;script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&amp;q=data+mining,+pattern+recognition,+machine+learning,+artificial+intelligence&amp;date=1/2014+12m&amp;cmpt=q&amp;tz&amp;tz&amp;content=1&amp;cid=TIMESERIES_GRAPH_0&amp;export=5&amp;w=680&amp;h=330"&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;div class="caption"&gt;2014년 기준 데이터마이닝, 패턴인식, 기계학습, 인공지능에 대한 구글 트렌드. 음? 그렇다면 &lt;a href="http://www.google.com/trends/explore#q=data%20mining%2C%20pattern%20recognition%2C%20machine%20learning%2C%20artificial%20intelligence%2C%20big%20data&amp;cmpt=q&amp;tz="&gt;빅데이터(big data)&lt;/a&gt;는? 데이터사이언스(data science)는?&lt;/div&gt;

&lt;p&gt;이 영역들은 각기 다른 탄생 배경을 가지고, 엄밀하게는 철학과 목적이 상당히 다르기도 하지만, 방법론의 측면에서는 상당히 유사해서 각 영역끼리 서로 배우는 점도 많지요.
사실 공부도 같은 책으로 많이 해요.
(심도있는 공부를 원하시는 분들은 아래 책들도 한 번 찾아보세요!)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ecx.images-amazon.com/images/I/612j5Uo43eL._AA160_.jpg"&gt;&lt;img src="images/book1.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/41LeU3HcBdL._AA160_.jpg"&gt;&lt;img src="images/book2.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/419Ml9MDMaL._AA160_.jpg"&gt;&lt;img src="images/book3.jpg" height="200px"&gt;&lt;/a&gt;
&lt;a href="http://ecx.images-amazon.com/images/I/51MucLjt9IL._AA160_.jpg"&gt;&lt;img src="images/book4.jpg" height="200px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;
iframe {
    width: 700px;
    height: 330px;
}
&lt;/style&gt;

&lt;h2&gt;데이터가 우리 삶을 돕는 다섯 가지 방법&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://readme.skplanet.com/?p=8870"&gt;미래 교통정보 예측: SK Planet T map&lt;/a&gt;&lt;br&gt;
    &lt;a href="http://i1.daumcdn.net/thumb/R750x0/?fname=http%3A%2F%2Fcfile26.uf.tistory.com%2Fimage%2F2204B946524405E408A53F"&gt;&lt;img src="images/tmap.png" width="500px"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://watcha.net"&gt;영화 추천: Frograms Watcha&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/watcha.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://google.com"&gt;문서 랭킹: Google&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/google.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://labs.naver.com/tech.html#multimedia_recognition"&gt;음악 인식: Naver 앱 음악 및 와인라벨 인식&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/naver.png" width="500px"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.apple.com/ios/siri/"&gt;질의응답 (QA): Apple Siri&lt;/a&gt;&lt;br&gt;
    &lt;img src="images/siri0.jpg" width="30%"&gt;
    &lt;img src="images/siri1.jpg" width="30%"&gt;
    &lt;img src="images/siri2.jpg" width="30%"&gt;&lt;/p&gt;
&lt;p&gt;cf. IBM Watson&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;데이터가 우리 삶을 바꿀 가지 방법&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Jarvis and Samantha&lt;br&gt;
    &lt;img src="images/jarvis.png" width="40%"&gt;
    &lt;img src="images/samantha.png" width="40%"&gt;
    &lt;!--
    &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/ZwOxM0-byvc" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
    --&gt;&lt;/li&gt;
&lt;li&gt;IOT&lt;br&gt;
    &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/NjYTzvAVozo?list=PLl-15sUN2G4eEY2VOqxMEazASNrlMF5FP" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;데이터마이너가 되면 좋은 점&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;세상의 다양한 면을, 다양한 관점에서 살펴볼 수 있습니다&lt;ul&gt;
&lt;li&gt;마케팅부터 시작해서,&lt;/li&gt;
&lt;li&gt;주가의 흐름을 예측하거나(금융),&lt;/li&gt;
&lt;li&gt;DNA 분석이나 MRI 영상을 분석하기도 하며(의료),&lt;/li&gt;
&lt;li&gt;디지털 카메라에서 얼굴 인식(기계)을 하기도 합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;사실상 직업이 매일 바뀌는 것이나 다름없음&lt;/li&gt;
&lt;li&gt;물론 그 외에도 우리가 상상할 수 있는 대부분의 영역에 데이터마이닝이 적용된다는 사실!&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;다음 시간 예고:
데이터마이닝을 할 때 가장 중요한 것은? Asking the right question.&lt;/p&gt;
&lt;/blockquote&gt;</summary><category term="seoultech"></category><category term="lectures"></category></entry><entry><title>Data Mining (Spring 2015)</title><link href="http://lucypark.kr/courses/2015-dm/index.html" rel="alternate"></link><updated>2015-03-02T00:00:00+09:00</updated><author><name>Lucy Park</name></author><id>tag:lucypark.kr,2015-03-02:courses/2015-dm/index.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;Welcome to Data Mining class of 2015!
Here you'll find all course materials, guides and schedules.
For discussions, please visit the class &lt;a href="http://eclass.seoultech.ac.kr/"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;데이터마이닝(data mining)이란 대용량 데이터베이스에 존재하는 데이터에서 관계, 패턴, 규칙 등을 찾아내고 모형화해서 의사결정을 돕는 유용한 정보로 변환하는 일련의 과정이다.
본 강좌에서는 기술 모델링(descriptive modeling)과 예측 모델링(predictive modeling)에 사용되는 탐색적 통계, 기계학습, 범주형 자료분석 기법들을 공부하고 응용사례 연구와 패키지를 이용한 프로젝트를 수행한다.&lt;/p&gt;
&lt;h3&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;한 가지 이상의 프로그래밍 언어에 대한 친숙함 (ex: R, Python, Java, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What you will learn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;데이터마이닝의 기본 개념, 구축 프로세스 학습 및 비즈니스 활용 사례 인식&lt;/li&gt;
&lt;li&gt;데이터마이닝의 주요 방법론의 이론적 토대 학습 및 응용 분야 학습&lt;/li&gt;
&lt;li&gt;데이터마이닝 분야에서 널리 사용되는 오픈소스 데이터 분석 언어인 &lt;a href="https://python.org/"&gt;파이썬&lt;/a&gt; 사용 방법 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Grading&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Assignments (30%)&lt;/li&gt;
&lt;li&gt;Final exam (30%)&lt;/li&gt;
&lt;li&gt;Term Project (40%)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Schedule&lt;/h2&gt;
&lt;p&gt;For assignment guidelines, visit the class &lt;a href="http://eclass.seoultech.ac.kr"&gt;e-class&lt;/a&gt;.&lt;/p&gt;
&lt;table id="schedule" class="table table-bordered"&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;th&gt;date&lt;/th&gt;&lt;th&gt;lecture&lt;/th&gt;&lt;/tr&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/06&lt;/td&gt;&lt;td&gt;&lt;a href="course-introduction.html"&gt;Course introduction&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="data-mining.html"&gt;Data mining&lt;/a&gt;&lt;li&gt;Assignment 0 (Due: 3/11) + Project proposal (Due: 3/18)&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/11&lt;/td&gt;&lt;td&gt;Due date: Assignment 0&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/13&lt;/td&gt;&lt;td&gt;&lt;a href="multiple-linear-regression.html"&gt;지도학습 1: Multiple linear regression&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="../tips/introduction-to-python.html"&gt;Introduction to Python&lt;/a&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/18&lt;/td&gt;&lt;td&gt;Due date: Project proposal, Fix project teams&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/20&lt;/td&gt;&lt;td&gt;&lt;a href="logistic-regression.html"&gt;지도학습 2: Logistic regression&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="assignments.html#assignment-1-분류기-비교"&gt;Assignment 1 (Due: 4/2)&lt;/a&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;3/27&lt;/td&gt;&lt;td&gt;지도학습 3: Decision tree + k-NN&lt;ul&gt;&lt;li&gt;In class project discussions&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;3/29&lt;/td&gt;&lt;td&gt;Due date: Choose team project topic&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;4/02&lt;/td&gt;&lt;td&gt;Due date: Assignment 1&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/03&lt;/td&gt;&lt;td&gt;텍스트 마이닝 1: Bag of words&lt;ul&gt;&lt;li&gt;Assignment 2 (Due: 4/16)&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/10&lt;/td&gt;&lt;td&gt;텍스트 마이닝 2: TF-IDF&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;4/16&lt;/td&gt;&lt;td&gt;Due date: Assignment 2&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/17&lt;/td&gt;&lt;td&gt;비지도학습 1: k-means + hierarchical clustering&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;4/24&lt;/td&gt;&lt;td&gt;비지도학습 2: Market basket analysis&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;4/30&lt;/td&gt;&lt;td&gt;Due date: Team project presentation slides&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/01&lt;/td&gt;&lt;td&gt;Team project presentations&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/08&lt;/td&gt;&lt;td&gt;지도학습 5: Artificial neural networks&lt;ul&gt;&lt;li&gt;Assignment 3 (Due: 6/4)&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/15&lt;/td&gt;&lt;td&gt;지도학습 6: Support vector machines&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/22&lt;/td&gt;&lt;td&gt;데이터 시각화&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;5/29&lt;/td&gt;&lt;td&gt;변수 선택과 차원 축소&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;6/04&lt;/td&gt;&lt;td&gt;Due date: Assignment 3&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6/05&lt;/td&gt;&lt;td&gt;대용량 데이터마이닝&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;6/12&lt;/td&gt;&lt;td&gt;Final Exam&lt;/td&gt;&lt;/tr&gt;
&lt;tr class="due-date"&gt;&lt;td&gt;6/14&lt;/td&gt;&lt;td&gt;Due date: Team project final report&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;</summary><category term="data"></category><category term="seoultech"></category><category term="lecturer"></category><category term="syllabus"></category></entry></feed>