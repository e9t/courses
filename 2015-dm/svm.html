<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Support vector machines — Courses</title>
	<meta name="description" content="Title: Support vector machines; Date: 2015-05-22; Author: Lucy Park; Courseid: 2015-dm; Metainfo: ">
	<meta name="author" content="Lucy Park">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="http://lucypark.kr/courses/theme/html5.js"></script>
		<![endif]-->
	<link href="http://lucypark.kr/courses/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="http://lucypark.kr/courses/theme/css/local.css" rel="stylesheet">
	<link href="http://lucypark.kr/courses/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1 class="margin-bottom-30"><a href="http://lucypark.kr/courses/">Courses</a></h1>
        <ul class="nav nav-tabs">
            <li role="presentation"><a href="http://lucypark.kr/courses/2015-dm">2015-dm</a></li>
            <li role="presentation"><a href="http://lucypark.kr/courses/2015-ba">2015-ba</a></li>
        </ul>
	</div>
	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="article-header">
        <h1 itemprop="name headline">Support vector machines</h1>
		<time datetime="2015-05-22T09:00:00+09:00" itemprop="datePublished">Fri 22 May 2015</time>
	</div>
    <div class="article-toc"><b>Contents</b><p></p></div>
    <div itemprop="articleBody" class="article-body">
        <h2>Class overview</h2>
<p>You learning to teach the computer to learn from data</p>
<ul>
<li>How?<ul>
<li>Find a <em>general rule</em> (i.e., 일반화 성능 향상, 오버피팅 방지)</li>
<li>That explains data given only as a sample of limited size</li>
<li>According to some measurement of accuracy or error</li>
<li>한마디로, finding <a href="https://www.goodreads.com/book/show/13588394-the-signal-and-the-noise">signals among the noise</a></li>
</ul>
</li>
<li>지금까지 배운 방법론들<ul>
<li>Supervised learning<ul>
<li>Data are sample of input-output pairs</li>
<li>Find input-output mapping</li>
<li>Regression, classification, etc</li>
</ul>
</li>
<li>Unsupervised learning<ul>
<li>Data are sample of objects</li>
<li>Find some common structure</li>
<li>Clustering, etc.</li>
</ul>
</li>
<li>Text mining</li>
</ul>
</li>
<li>오늘 배울 것: 여전히 핫한 알고리즘, SVM, and yet another supervised learning algorithm</li>
<li>앞으로 남은 세 시간:<ul>
<li><code>classes[-3]</code>: Semisupervised learning + A touch of visualization</li>
<li><code>classes[-2]</code>: Big data technologies: Hadoop &amp; spark + Tips for your exam</li>
<li><code>classes[-1]</code>: 대망의 기말고사</li>
</ul>
</li>
</ul>
<h2>SVM: Support vector machines</h2>
<script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&q=neural+networks,+svm,+deep+learning&cmpt=q&tz&tz&content=1&cid=TIMESERIES_GRAPH_0&export=5&w=500&h=330"></script>

<!-- cf. <s>single valuable man ?</s> -->

<h3>Classification examples</h3>
<ol>
<li>Sheep vector machines<ul>
<li>Using the existing sheep distributions (the training set), determine whether the new sheep belongs with the white sheep or the black sheep.<br>
<img src="images/sheep.png" width="300px"></li>
</ul>
</li>
<li>Spam filtering<ul>
<li>Using word occurrences in existing email documents, determine whether a new email is spam or ham.<br>
<img src="images/spam.png" width="500px"></li>
<li>Instance space: $x \in X$  ($|X| = n$ data points)<ul>
<li>Binary or real-valued feature vector $x$ of word occurrences</li>
<li>$d$ features (words + other things, d~100,000+)</li>
</ul>
</li>
<li>Class: $y \in Y$<ul>
<li>$y$: Spam (+1), Ham (-1)</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><table class="table"><tr><th>viagra</th><th>learning</th><th>the</th><th>dating</th><th>nigeria</th><th>is_spam</th></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td></tr></table></p>
<h3>Linear binary classification</h3>
<ul>
<li>There may exist many solutions that separate the classes exactly</li>
<li>Usually, we find the one that will give the smallest generalization error</li>
<li>This is the problem of choosing the or <strong>decision boundary</strong>, or <strong>hyperplane</strong><br>
    <img src="images/decision_boundary.png" width="400px"></li>
<li>Input: Binary/real valued vectors $x$ and labels $y$</li>
<li>Goal: Find real valued vector $w$<br>
<img src="images/goal.png" width="400px"></li>
<li>Each feature has a weight $w_i$<ul>
<li>Prediction is based on the weighted sum: $f(x) = \sum w_i x_i = w \cdot x$</li>
<li>If the f(x) is<ul>
<li>Positive: Predict +1 (i.e., is sheep, is spam)</li>
<li>Negative: Predict -1 (i.e., is not sheep, is ham)<br>
<img src="images/predict.png" width="400px"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>SVM, the maximal margin classifier</h3>
<ul>
<li>Idea:<ul>
<li>Distance from the separating hyperplane corresponds to the "confidence" of prediction</li>
<li>In the image below, we are more sure about the class of A and B than of C<br>
<img src="images/abc.png" width="200px"></li>
</ul>
</li>
<li>SVM finds the decision boundary with concept of maximizing this distance, or <strong>margins</strong><ul>
<li>Margin $\gamma$: The perpendicular distance between the decision boundary and the closest of the data points (left figure below).
<!--
The reason we define margin this way is due to theoretical convenience and existence of
generalization error bounds that depend on the value of margin. - Jurafsky
--></li>
<li>Support vectors: Maximizing the margin leads to a particular subset of existing data points (right figure below).<br>
<img src="images/svm.png" width="600px"></li>
</ul>
</li>
<li>Why is maximizing $\gamma$ a good idea?<ul>
<li>Remember: Dot product $A \cdot B = |A||B|cos\theta$<br>
<img src="images/cosine.png" width="200px"></li>
<li>Let:<ul>
<li>Line $L = w \cdot x + b = 0$</li>
<li>Weight $w = [w_1, w_2]$</li>
<li>Point $A = [x_1^{(A)}, x_2^{(A)}]$</li>
<li>Point $M = [x_1^{{(M)}}, x_2^{{(M)}}]$<br>
<img src="images/margin.png" width="200px"></li>
</ul>
</li>
<li>Then the distance between A, L:
    $$\begin{align}
    d(A, L) &amp; = |AH|\\
        &amp; = |(A-M) \cdot w|\\
        &amp; = |(x_1^{(A)} - x_1^{(M)}) w_1 + (x_2^{(A)} - x_2^{(M)}) w_2|\\
        &amp; = x_1^{(A)} w_1 + x_2^{(A)} w_2 + b\\
        &amp; = w \cdot A + b
    \end{align}$$</li>
<li>Prediction = $sign(w \cdot x + b)$</li>
<li>"Confidence" = $(w \cdot x + b)y$</li>
<li>For i-th data point: $\gamma_i = (w \cdot x^{(i)} + b)y^{(i)}$</li>
<li>Therefore, the objective function becomes:<br><ul>
<li>max $\gamma$ s.t., $y^{(i)}(w \cdot x^{(i)} + b) \geq \gamma$ ($\forall i$)</li>
</ul>
</li>
<li>Good according to 1) intuition 2) theory and 3) practice</li>
</ul>
</li>
<li>Normalized weights<ul>
<li>Problem: With this equation, scaling $w$ increases margin! (i.e., $w$ can be arbitrarily large)<ul>
<li>If $(w \cdot x + b)y = \gamma$, then $(2w \cdot x + 2b)y = 2\gamma$</li>
</ul>
</li>
<li>Solution: work with normalized $w$, and require support vectors to be on the margin<ul>
<li>$\gamma = (\frac{w}{|w|} \cdot x + b)y$</li>
<li>$w \cdot x^{(i)} + b = \pm 1$</li>
</ul>
</li>
</ul>
</li>
<li>Margin maximization == weight minimization?<br>
    <img src="images/maxmin.png" width="300px"><ul>
<li>We know three things<ul>
<li>$x^{(1)} = x^{(2)} + 2 \gamma \frac{w}{|w|}$</li>
<li>$w \cdot x^{(1)} + b = +1$</li>
<li>$w \cdot x^{(2)} + b = -1$</li>
</ul>
</li>
<li>Therefore<ul>
<li>$w \cdot x^{(1)} + b = +1$</li>
<li>$w (x^{(2)} + 2 \gamma \frac{w}{|w|}) + b = +1$</li>
<li>$(w \cdot x^{(2)} + b) + 2 \gamma \frac{w}{|w|} = +1$</li>
<li>$\gamma = \frac{1}{|w|}$</li>
</ul>
</li>
<li>max $\gamma \thickapprox$ max $\frac{1}{|w|} \thickapprox$ min $|w| \thickapprox$ min $\frac{1}{2} |w|^2$</li>
<li>Which finally gives<ul>
<li>min $\frac{1}{2} |w|^2$ s.t., $y^{(i)}(w \cdot x^{(i)} + b) \geq 1$ ($\forall i$)</li>
<li>This is called SVM with "hard" constraints</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Soft margin SVMs</h3>
<ul>
<li>Relax constraints<ul>
<li>Allowing the margin constraints to be violated</li>
<li>In other words, allow some of the training data points to be misclassified</li>
</ul>
</li>
<li>min $\frac{1}{2} |w|^2 + C \sum_{i=1}^n \xi_i$ s.t., $y^{(i)}(w \cdot x^{(i)} + b) \geq 1 - \xi_i$ ($\forall i$)<br>
    <img src="images/xi.png" width="300px"></li>
</ul>
<h3><a href="http://en.wikipedia.org/wiki/Kernel_method">Kernel methods</a></h3>
<blockquote>
<p>Warning: NOT related to shell/kernels in the OS</p>
</blockquote>
<ul>
<li>Life is not so easy, not all problems are linearly separable</li>
<li>If so, choose a mapping to some (high dimensional) dot-product space, namely the <em>feature space</em>: $\Phi: X \to H$</li>
</ul>
<p><img src="images/feature_space.png" width="400px"></p>
<ul>
<li>Mercer's condition<ul>
<li>If a symmetric function $K(x, y)$ satisfies $\sum_{i,j=1}^{M} a_ia_jK(x_i,x_j) \geq 0$<br>
  for all $M \in \mathbb{N}, x_i, a_i \in \mathbb{R}$<br>
  there exists a mapping function $\Phi$ that maps x into the dot-product feature space and $K(x, y) = &lt;\Phi(x), \Phi(y)&gt;$ and vice versa.</li>
</ul>
</li>
<li>Function $K$ is called the <strong>kernel</strong>.</li>
<li>Types of kernels<ul>
<li>Linear kernels: $K(x, y) = &lt;x, y&gt;$</li>
<li>Polynomial kernels: $K(x, y) = (&lt; x, y&gt; + 1)^d$ for $d = 2$</li>
<li>RBF kernels: $K(x, y) = exp(-\frac{||x-y||^2}{d^2})$</li>
<li>...and more!<ul>
<li>Kernels on various objects, such as graphs, strings, texts, etc.</li>
<li>Enable us to use dot-product algorithms</li>
<li>Measure of similarity</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Programming SVMs</h2>
<p>Go to the <a href="http://scikit-learn.org/stable/modules/svm.html">SVM documents</a> in the Scikit-learn webpage.</p>
<ul>
<li>SVC: Support vector <em>classification</em></li>
<li>SVR: Support vector <em>regression</em></li>
<li>Also see: <a href="http://www.quora.com/What-is-the-difference-between-C-SVM-and-nu-SVM">What is the difference between C-SVM and nu-SVM?</a></li>
</ul>
<h2>References</h2>
<ul>
<li>Wikipedia, <a href="http://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine</a></li>
<li>Scikit-learn, <a href="http://scikit-learn.org/stable/modules/svm.html">Support Vector Machines</a></li>
<li>Andrew Ng, <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">CS229 Lecture notes: Support vector machines</a></li>
<li>Petra Kudova, <a href="http://www.cs.cas.cz/petra/slides/svm.pdf">Learning with kernels and SVM</a></li>
<li><a href="http://www.support-vector-machines.org/">support-vector-machines.org</a> (Sometimes, algorithms have websites of their own! See <a href="https://twitter.com/echojuliett/status/488991816595697664">here</a> for more of them.)</li>
</ul>
<blockquote>
<p>Many contents in courtesy of <a href="http://cs.stanford.edu/people/jure/">Jure Leskovec</a> and <a href="http://www.cs.cas.cz/petra/">Petra Kudova</a></p>
</blockquote>

    </div>
    <div class="footnote">
        <div class="social-links pull-right">
            <div id="facebook-like">
                <div id="fb-root"></div>
                <script>(function(d, s, id) {
                  var js, fjs = d.getElementsByTagName(s)[0];
                  if (d.getElementById(id)) return;
                  js = d.createElement(s); js.id = id;
                  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=357567114353849";
                  fjs.parentNode.insertBefore(js, fjs);
                }(document, 'script', 'facebook-jssdk'));</script>
                <div class="fb-like" data-href="http://lucypark.kr/courses/2015-dm/svm.html" data-send="false" data-layout="button_count" data-width="450" data-show-faces="false"></div>
            </div>
            <div id="twitter-tweet">
                <a href="https://twitter.com/share" class="twitter-share-button" data-lang="en"></a>
                <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.    insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
            </div>
        </div>
        <p>
            Author:
            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                <a href="http://dm.snu.ac.kr/~epark"><span itemprop="name">Lucy Park</span></a>
            </span>
        </p>
        <p>
            Category:
            <span itemprop="articleSection">
                <a href="http://lucypark.kr/courses/category/2015-dm.html" rel="category">2015-dm</a>
            </span>
        </p>
        <p>
            Tags:
            <span itemprop="keywords">
                <a href="http://lucypark.kr/courses/tag/lectures.html" rel="tag">lectures</a>
            </span>
            <span itemprop="keywords">
                <a href="http://lucypark.kr/courses/tag/svm.html" rel="tag">svm</a>
            </span>
        </p>
    </div>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="center-block aw-bottom">
            Built by <a href="http://dm.snu.ac.kr/~epark">Lucy Park</a>.<br>
            Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License v2.0</a>, document under <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.<br>
 The code for this site is located at <a href="http://github.com/e9t/courses">GitHub</a>.             <span class="pull-right margin-top-30">
                <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
                <a href="http://lucypark.kr/courses/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a>
            </span>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
	</div>
</div>
<!-- JavaScript -->
<script src="//code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});

// add 'active' class to current menu
$(function(){
  if (document.location.hostname == "localhost") {
    var idx = 1;
    var activePage = location.pathname.split("/")[idx];
  } else {
    var idx = 4;
    var activePage = location.href.split("/")[idx];
  }
  $('.nav li a').each(function() {
    var currentPage = $(this).attr('href').split("/")[idx];
    if (activePage == currentPage) {
      $(this).parent().addClass('active');
    }
  });
});
</script>
<script>
    function hyphenate(str) {
        return str.toLowerCase().replace(/[?!:'"\[\]()%.,]/gi, '').replace(/\s/g, '-');
    }

    String.prototype.repeat = function(num) { return new Array(num + 1).join(this); }

    // create toc
    var t = '';
    var concat = '';
    var headers = $('.article-body :header');
    headers.each(function() {
        var hyphenated = hyphenate($(this).text());
            $(this).attr('id', hyphenated);
        }
    );
    for (var i=0; i<headers.length; i++) {
        depth = parseInt(headers[i].tagName.substr(-1));
        if (depth < 4) {
            t = headers[i].textContent;
            concat += '&nbsp;&nbsp;&nbsp;&nbsp;'.repeat(parseInt(depth)-2) + '- <a href="#' + hyphenate(t) + '">' + t + '</a><br>';
        }
    }
    $(".article-toc p").append(concat);

    // scroll animation
    $(document).ready(function(){
        $('a[href^="#"]').on('click',function (e) {
            e.preventDefault();

            var target = this.hash;
            var $target = $(target);

            $('html, body').stop().animate({
                'scrollTop': $target.offset().top
            }, 300, 'swing', function () {
                window.location.hash = target;
            });
        });
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$', '$$'], ["\\[", "\\]"]],
    }});
</script>
<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', UA-4510606-3, 'aboutwilson.net');
ga('require', 'displayfeatures');
ga('require', 'linkid', 'linkid.js');
ga('send', 'pageview');

</script>
</body>
</html>