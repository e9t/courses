<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Mining English and Korean text with Python — Courses</title>
	<meta name="description" content="Title: Mining English and Korean text with Python; Date: 2015-03-27; Author: Lucy Park; Courseid: 2015-ba; Metainfo: ">
	<meta name="author" content="Lucy Park">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
		<script src="http://lucypark.kr/courses/theme/html5.js"></script>
		<![endif]-->
	<link href="http://lucypark.kr/courses/theme/css/ipython.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="//maxcdn.bootstrapcdn.com/bootswatch/3.2.0/simplex/bootstrap.min.css" rel="stylesheet">
	<link href="http://lucypark.kr/courses/theme/css/local.css" rel="stylesheet">
	<link href="http://lucypark.kr/courses/theme/css/pygments.css" rel="stylesheet">
</head>
<body>
<div class="container">
	<div class="page-header">
		<h1 class="margin-bottom-30"><a href="http://lucypark.kr/courses/">Courses</a></h1>
        <ul class="nav nav-tabs">
            <li role="presentation"><a href="http://lucypark.kr/courses/2015-dm">2015-dm</a></li>
            <li role="presentation"><a href="http://lucypark.kr/courses/2015-ba">2015-ba</a></li>
        </ul>
	</div>
	<div class="row">
		<div class="col-md-10 col-md-offset-1">
<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="article-header">
        <h1 itemprop="name headline">Mining English and Korean text with Python</h1>
		<time datetime="2015-03-27T15:00:00+09:00" itemprop="datePublished">Fri 27 March 2015</time>
	</div>
    <div class="article-toc"><b>Contents</b><p></p></div>
    <div itemprop="articleBody" class="article-body">
        <blockquote>
<p>We use Python 3 in this tutorial, but provide minimal guidelines for Python 2.</p>
</blockquote>
<h2>Terminologies</h2>
<p><table class="table"><tr><th>English</th><th>한국어</th><th>Description</th></tr><tr><td>Document</td><td>문서</td><td>-</td></tr><tr><td>Corpus</td><td>말뭉치</td><td>A set of documents</td></tr><tr><td>Token</td><td>토큰</td><td>Meaningful elements in a text such as words or phrases or symbols</td></tr><tr><td>Morphemes</td><td>형태소</td><td>Smallest meaningful unit in a language</td></tr><tr><td>POS</td><td>품사</td><td>Part-of-speech (ex: Nouns)</td></tr><tr><td>Classification</td><td>분류</td><td>A supervised learning task where $X$ and $y$ are given and $y$ is a set of discrete classes</td></tr><tr><td>Clustering</td><td>군집화</td><td>An unsupervised learning task where $X$ is given</td></tr></table></p>
<h2>Text analysis process</h2>
<ol>
<li>Load text</li>
<li>Tokenize text (ex: stemming, morph analyzing)</li>
<li>Tag tokens (ex: POS, NER)</li>
<li>Token(Feature) selection and/or filter/rank tokens (ex: stopword removal, TF-IDF)</li>
<li>...and so on (ex: calculate word/document similarities, cluster documents)</li>
</ol>
<h2>Python Packages for Text Mining and NLP</h2>
<p>...that we use in this tutorial.</p>
<ol>
<li>
<p><a href="http://nltk.org">NLTK</a>: Provides modules for text analysis (mostly language independent)</p>
<div class="highlight"><pre>pip install nltk
</pre></div>


<ul>
<li>
<p><a href="http://www.nltk.org/book/ch02.html">Text corpora</a></p>
<div class="highlight"><pre><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">&#39;gutenberg&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s">&#39;maxent_treebank_pos_tagger&#39;</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p><a href="http://www.nltk.org/api/nltk.tag.html">Word POS, NER classification</a></p>
</li>
<li><a href="http://www.nltk.org/book/ch06.html">Document classification</a></li>
</ul>
</li>
<li>
<p><a href="http://konlpy.org">KoNLPy</a>: Provides modules for Korean text analysis</p>
<div class="highlight"><pre>pip install konlpy
</pre></div>


<ul>
<li><a href="http://konlpy.org/en/latest/data/#corpora">Text corpora</a></li>
<li><a href="http://konlpy.org/en/latest/api/konlpy.tag/">Word POS classification</a><ul>
<li>Hannanum</li>
<li>Kkma</li>
<li>Mecab</li>
<li>Komoran</li>
<li>Twitter</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="http//radimrehurek.com/gensim/">Gensim</a>: Provides modules for topic modeling and calculating similarities among documents</p>
<div class="highlight"><pre>pip install -U gensim
</pre></div>


<ul>
<li>Topic modeling<ul>
<li><a href="http://radimrehurek.com/gensim/models/ldamodel.html">Latent Dirichlet allocation (LDA)</a></li>
<li><a href="http://radimrehurek.com/gensim/models/lsimodel.html">Latent semantic indexing (LSI)</a></li>
<li><a href="http://radimrehurek.com/gensim/models/hdpmodel.html">Hierarchical Dirichlet process (HDP)</a></li>
</ul>
</li>
<li>Word embedding<ul>
<li><a href="radimrehurek.com/gensim/models/word2vec.html">word2vec</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="https://github.com/ryanmcgrath/twython">Twython</a>: Provides easy access to Twitter API</p>
<div class="highlight"><pre>pip install twython
</pre></div>


<ul>
<li>Example: Getting "Samsung (삼성)" related tweets<div class="highlight"><pre><span class="kn">from</span> <span class="nn">twython</span> <span class="kn">import</span> <span class="n">Twython</span>
<span class="kn">import</span> <span class="nn">settings</span> <span class="kn">as</span> <span class="nn">s</span>    <span class="c"># Create a file named settings.py, and put oauth KEY values inside</span>
<span class="n">twitter</span> <span class="o">=</span> <span class="n">Twython</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">APP_KEY</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">APP_SECRET</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">OAUTH_TOKEN</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">OAUTH_TOKEN_SECRET</span><span class="p">)</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">twitter</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">q</span><span class="o">=</span><span class="s">&#39;삼성&#39;</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">t</span><span class="p">[</span><span class="s">&#39;user&#39;</span><span class="p">][</span><span class="s">&#39;screen_name&#39;</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="s">&#39;text&#39;</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="s">&#39;created_at&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">[</span><span class="s">&#39;statuses&#39;</span><span class="p">]]</span>
</pre></div>


</li>
</ul>
</li>
</ol>
<h2>Text exploration</h2>
<h3>1. Read document</h3>
<p>As example documents, we select
<a href="http://www.gutenberg.org/ebooks/158">Jane Austen's Emma</a> for English,
and <a href="http://pokr.kr/bill/1809890">Korea National Assembly's bill number 1809890</a> for Korean.
Otherwise, you can use a document of your own with <code>open('some_file.txt').read()</code>.</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">gutenberg</span>   <span class="c"># Docs from project gutenberg.org</span>
<span class="n">files_en</span> <span class="o">=</span> <span class="n">gutenberg</span><span class="o">.</span><span class="n">fileids</span><span class="p">()</span>      <span class="c"># Get file ids</span>
<span class="n">doc_en</span> <span class="o">=</span> <span class="n">gutenberg</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s">&#39;austen-emma.txt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.corpus</span> <span class="kn">import</span> <span class="n">kobill</span>    <span class="c"># Docs from pokr.kr/bill</span>
<span class="n">files_ko</span> <span class="o">=</span> <span class="n">kobill</span><span class="o">.</span><span class="n">fileids</span><span class="p">()</span>         <span class="c"># Get file ids</span>
<span class="n">doc_ko</span> <span class="o">=</span> <span class="n">kobill</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s">&#39;1809890.txt&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>


</li>
</ul>
<h3>2. Tokenize</h3>
<p>There are numerous ways to tokenize a document.</p>
<p>Here, we use <code>nltk.regexp_tokenize</code> for English,
<code>konlpy.tag.Twitter.morph</code> for Korean text.</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">regexp_tokenize</span>
<span class="n">pattern</span> <span class="o">=</span> <span class="s">r&#39;&#39;&#39;(?x) ([A-Z]\.)+ | \w+(-\w+)* | \$?\d+(\.\d+)?%? | \.\.\. | [][.,;&quot;&#39;?():-_`]&#39;&#39;&#39;</span>
<span class="n">tokens_en</span> <span class="o">=</span> <span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">doc_en</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.tag</span> <span class="kn">import</span> <span class="n">Twitter</span><span class="p">;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Twitter</span><span class="p">()</span>
<span class="n">tokens_ko</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">morphs</span><span class="p">(</span><span class="n">doc_ko</span><span class="p">)</span>
</pre></div>


</li>
</ul>
<h3>3. Load tokens with <code>nltk.Text()</code></h3>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">en</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">Text</span><span class="p">(</span><span class="n">tokens_en</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">ko</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">Text</span><span class="p">(</span><span class="n">tokens_ko</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">&#39;대한민국 국회 의안 제 1809890호&#39;</span><span class="p">)</span>   <span class="c"># For Python 2, input `name` as u&#39;유니코드&#39;</span>
</pre></div>


</li>
</ul>
<p><code>nltk.Text()</code> is a convenient way to explore a current document.
For Python 2, <code>name</code> has to be input as u'유니코드'.
If you are using Python 2, use u'유니코드' for input of all following Korean text.</p>
<ol>
<li>
<p>Tokens</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">tokens</span><span class="p">))</span>       <span class="c"># returns number of tokens (document length)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">en</span><span class="o">.</span><span class="n">tokens</span><span class="p">)))</span>  <span class="c"># returns number of unique tokens</span>
<span class="n">en</span><span class="o">.</span><span class="n">vocab</span><span class="p">()</span>                  <span class="c"># returns frequency distribution</span>
</pre></div>


<p><pre class="result">
191061
7927
FreqDist({',': 12018, '.': 8853, 'to': 5127, 'the': 4844, 'and': 4653, 'of': 4278, '"': 4187, 'I': 3177, 'a': 3000, 'was': 2385, ...})
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ko</span><span class="o">.</span><span class="n">tokens</span><span class="p">))</span>       <span class="c"># returns number of tokens (document length)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">ko</span><span class="o">.</span><span class="n">tokens</span><span class="p">)))</span>  <span class="c"># returns number of unique tokens</span>
<span class="n">ko</span><span class="o">.</span><span class="n">vocab</span><span class="p">()</span>                  <span class="c"># returns frequency distribution</span>
</pre></div>


<p><pre class="result">
1707
476
FreqDist({'.': 61, '의': 46, '육아휴직': 38, '을': 34, '(': 27, ',': 26, '이': 26, ')': 26, '에': 24, '자': 24, ...})
</pre></p>
</li>
</ul>
</li>
<li>
<p>Plot frequency distributions</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>     <span class="c"># Plot sorted frequency of top 50 tokens</span>
</pre></div>


<p><img src="images/fdist_en.png" width="500px"></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">ko</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>     <span class="c"># Plot sorted frequency of top 50 tokens</span>
</pre></div>


<p><img src="images/fdist_ko.png" width="500px"></p>
</li>
</ul>
<blockquote>
<p><strong>Tip</strong>: To save a plot programmably, and not through the GUI, overwrite <code>pylab.show</code> with <code>pylab.savefig</code> before drawing the plot (<a href="http://stackoverflow.com/questions/27392390/how-do-i-send-nltk-plots-to-files">reference</a>):
<pre>
from matplotlib import pylab
pylab.show = lambda: pylab.savefig('some_filename.png')
</pre></p>
<p><strong>Troubleshooting</strong>: For those who see rectangles instead of letters in the saved plot file, include the following configurations before drawing the plot:
<pre>
from matplotlib import font_manager, rc
font_fname = 'c:/windows/fonts/gulim.ttc'     # A font of your choice
font_name = font_manager.FontProperties(fname=font_fname).get_name()
rc('font', family=font_name)
</pre></p>
<p>Some example fonts:</p>
<ul>
<li>Mac OS: <code>/Library/Fonts/AppleGothic.ttf</code></li>
</ul>
</blockquote>
</li>
<li>
<p>Count</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s">&#39;Emma&#39;</span><span class="p">)</span>        <span class="c"># Counts occurrences</span>
</pre></div>


<p><pre class="result">
865
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">ko</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s">&#39;초등학교&#39;</span><span class="p">)</span>   <span class="c"># Counts occurrences</span>
</pre></div>


<p><pre class="result">
6
</pre></p>
</li>
</ul>
</li>
<li>
<p>Dispersion plot</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">dispersion_plot</span><span class="p">([</span><span class="s">&#39;Emma&#39;</span><span class="p">,</span> <span class="s">&#39;Frank&#39;</span><span class="p">,</span> <span class="s">&#39;Jane&#39;</span><span class="p">])</span>
</pre></div>


<p><img src="images/disp_en.png" width="500px"></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">ko</span><span class="o">.</span><span class="n">dispersion_plot</span><span class="p">([</span><span class="s">&#39;육아휴직&#39;</span><span class="p">,</span> <span class="s">&#39;초등학교&#39;</span><span class="p">,</span> <span class="s">&#39;공무원&#39;</span><span class="p">])</span>
</pre></div>


<p><img src="images/disp_ko.png" width="500px"></p>
</li>
</ul>
</li>
<li>
<p>Concordance</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">concordance</span><span class="p">(</span><span class="s">&#39;Emma&#39;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
Displaying 5 of 865 matches:
                                     Emma by Jane Austen 1816 ] VOLUME I CHAPT
                                     Emma Woodhouse , handsome , clever , and 
both daughters , but particularly of Emma . Between <em>them</em> it was more the int
 friend very mutually attached , and Emma doing just what she liked ; highly e
r own . The real evils , indeed , of Emma ' s situation were the power of havi
</pre></p>
</li>
<li>
<p>Korean (or, use <a href="http://konlpy.org/en/v0.4.3/api/konlpy/#konlpy.utils.concordance">konlpy.utils.concordance</a>)</p>
<div class="highlight"><pre><span class="n">ko</span><span class="o">.</span><span class="n">concordance</span><span class="p">(</span><span class="s">&#39;초등학교&#39;</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
Displaying 6 of 6 matches:
 ․ 김정훈 김학송 의원 ( 10 인 ) 제안 이유 및 주요 내용 초등학교 저학년 의 경우 에도 부모 의 따뜻한 사랑 과 보살핌 이 필요 한
 을 할 수 있는 자녀 의 나이 는 만 6 세 이하 로 되어 있어 초등학교 저학년 인 자녀 를 돌보기 위해서 는 해당 부모님 은 일자리 를 
 다 . 제 63 조제 2 항제 4 호 중 “ 만 6 세 이하 의 초등학교 취학 전 자녀 를 ” 을 “ 만 8 세 이하 ( 취학 중인 경우 
 전 자녀 를 ” 을 “ 만 8 세 이하 ( 취학 중인 경우 에는 초등학교 2 학년 이하 를 말한 다 ) 의 자녀 를 ” 로 한 다 . 부 
 . ∼ 3 . ( 현행 과 같 음 ) 4 . 만 6 세 이하 의 초등학교 취 4 . 만 8 세 이하 ( 취학 중인 경우 학 전 자녀 를 양
세 이하 ( 취학 중인 경우 학 전 자녀 를 양육 하기 위하 에는 초등학교 2 학년 이하 를 여 필요하거 나 여자 공무원 이 말한 다 ) 의
</pre></p>
</li>
</ul>
</li>
<li>
<p>Find similar words</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">similar</span><span class="p">(</span><span class="s">&#39;Emma&#39;</span><span class="p">)</span>
<span class="n">en</span><span class="o">.</span><span class="n">similar</span><span class="p">(</span><span class="s">&#39;Frank&#39;</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
she it he i harriet you her jane him that me and all they them there herself was hartfield be
mr mrs emma harriet you it her she he him hartfield them jane that isabella all herself look i me
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">ko</span><span class="o">.</span><span class="n">similar</span><span class="p">(</span><span class="s">&#39;자녀&#39;</span><span class="p">)</span>
<span class="n">ko</span><span class="o">.</span><span class="n">similar</span><span class="p">(</span><span class="s">&#39;육아휴직&#39;</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
논의
None
</pre></p>
</li>
</ul>
</li>
<li>
<p>Collocations</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">collocations</span><span class="p">()</span>
</pre></div>


<p><pre class="result">
Frank Churchill; Miss Woodhouse; Miss Bates; Jane Fairfax; Miss
Fairfax; every thing; young man; every body; great deal; dare say;
John Knightley; Maple Grove; Miss Smith; Miss Taylor; Robert Martin;
Colonel Campbell; Box Hill; said Emma; Harriet Smith; William Larkins
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">en</span><span class="o">.</span><span class="n">collocations</span><span class="p">()</span>
</pre></div>


<p><pre class="result">
초등학교 저학년; 육아휴직 대상
</pre></p>
</li>
</ul>
</li>
</ol>
<!--
1. Common contexts
    - English

            :::python
            en.common_contexts(['Emma', 'Frank'])

        <pre class="result">
        that_could that_s for_i and_and and_was between_and on_s of_s to_it in_s
        </pre>

    - Korean

            :::python
            ko.common_contexts(['육아휴직'])

        <pre class="result">
        따라서_이 에서_급 p_대상자 받는_자 경우_급여 으로_기간 n_급 위하여_을 인_자 대비하여_자 와_자 따라_신청 표_급여
        에게_자 에는_자 근로자_가능 평균_급여 이며_에 에_자 가_을
        </pre>
-->

<p>For more information on <code>nltk.Text()</code>, see the <a href="http://www.nltk.org/_modules/nltk/text.html#Text">source code</a> or <a href="http://www.nltk.org/api/nltk.html#nltk.text.Text">API</a>.</p>
<h2>Tagging and chunking</h2>
<p>Until now, we used delimited text, namely <em>tokens</em>, to explore our sample document.
Now let's classify words into given classes, namely <em>part-of-speech tags</em>, and chunk text into larger pieces.</p>
<h3>1. POS tagging</h3>
<p>There are numerous ways of tagging a text.
Among them, the most frequently used, and developed way of tagging is arguably POS tagging.</p>
<p>Since one document is too long to observe a parsed structure,
lets use one short sentence for each language.</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">tokens</span> <span class="o">=</span> <span class="s">&quot;The little yellow dog barked at the Persian cat&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">tags_en</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[('The', 'DT'),
 ('little', 'JJ'),
 ('yellow', 'NN'),
 ('dog', 'NN'),
 ('barked', 'VBD'),
 ('at', 'IN'),
 ('the', 'DT'),
 ('Persian', 'NNP'),
 ('cat', 'NN')]
</pre></p>
<ul>
<li>It is also possible to use the famous <a href="http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford">Stanford POS tagger with NLTK</a>, with <code>from nltk.tag.stanford import POSTagger</code></li>
</ul>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.tag</span> <span class="kn">import</span> <span class="n">Twitter</span><span class="p">;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Twitter</span><span class="p">()</span>
<span class="n">tags_ko</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">pos</span><span class="p">(</span><span class="s">&quot;작고 노란 강아지가 페르시안 고양이에게 짖었다&quot;</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[('작고', 'Noun'),
 ('노란', 'Adjective'),
 ('강아지', 'Noun'),
 ('가', 'Josa'),
 ('페르시안', 'Noun'),
 ('고양이', 'Noun'),
 ('에게', 'Josa'),
 ('짖었', 'Noun'),
 ('다', 'Josa')]
</pre></p>
</li>
</ul>
<h3>2. Noun phrase chunking</h3>
<p><a href="http://www.nltk.org/api/nltk.chunk.html#nltk.chunk.regexp.RegexpParser"><code>nltk.RegexpParser()</code></a> is a great way to start chunking.</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">parser_en</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">RegexpParser</span><span class="p">(</span><span class="s">&quot;NP: {&lt;DT&gt;?&lt;JJ&gt;?&lt;NN.*&gt;*}&quot;</span><span class="p">)</span>
<span class="n">chunks_en</span> <span class="o">=</span> <span class="n">parser_en</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">tags_en</span><span class="p">)</span>
<span class="n">chunks_en</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</pre></div>


<p><img src="images/tree_en.png" width="500px"></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">parser_ko</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">RegexpParser</span><span class="p">(</span><span class="s">&quot;NP: {&lt;Adjective&gt;*&lt;Noun&gt;*}&quot;</span><span class="p">)</span>
<span class="n">chunks_ko</span> <span class="o">=</span> <span class="n">parser_ko</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">tags_ko</span><span class="p">)</span>
<span class="n">chunks_ko</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</pre></div>


<p><img src="images/tree_ko.png" width="700px"></p>
</li>
</ul>
<p>For more information on chunking, refer to <a href="http://www.nltk.org/book/ch07.html">Extracting Information from Text</a> for English, and <a href="http://konlpy.org/en/v0.4.3/examples/chunking/">Chunking</a> for Korean.</p>
<h2>Topic modeling</h2>
<ul>
<li>Topic modeling in a nutshell<br>
    <img src="images/topic-modeling.png" width="600px"></li>
<li>
<p>History<br>
    <img src="images/tm-history.png" width="600px"></p>
<ul>
<li>LSI: Learns latent topics by performing a matrix decomposition (SVD) on the term-document matrix</li>
<li>LDA: A generative probabilistic model, that assumes a Dirichelt prior over the latent topics</li>
<li>HDP: A natural nonparametric generalization of LDA, where the number of topics can be unbounded ant learnt from data</li>
</ul>
</li>
</ul>
<h3>1. Preprocessing</h3>
<ol>
<li>
<p>Load documents</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">reuters</span>
<span class="n">docs_en</span> <span class="o">=</span> <span class="p">[</span><span class="n">reuters</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">()]</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.corpus</span> <span class="kn">import</span> <span class="n">kobill</span>
<span class="n">docs_ko</span> <span class="o">=</span> <span class="p">[</span><span class="n">kobill</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kobill</span><span class="o">.</span><span class="n">fileids</span><span class="p">()]</span>
</pre></div>


</li>
</ul>
</li>
<li>
<p>Tokenize</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">texts_en</span> <span class="o">=</span> <span class="n">docs_en</span> <span class="c"># because we loaded tokenized documents in step 1</span>
<span class="k">print</span><span class="p">(</span><span class="n">texts_en</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<p><pre class="result">
['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.tag</span> <span class="kn">import</span> <span class="n">Twitter</span><span class="p">;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Twitter</span><span class="p">()</span>
<span class="n">pos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="p">[</span><span class="s">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">pos</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">stem</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
<span class="n">texts_ko</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs_ko</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">texts_ko</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<p><pre class="result">
['지방공무원법/Noun', '일부/Noun', '개정/Noun', '법률/Noun', '안/Noun', '(/Punctuation', '정의화/Noun', '의원/Noun', ...]
</pre></p>
</li>
</ul>
</li>
<li>
<p>Encode tokens to integers</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span>
<span class="n">dictionary_en</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">texts_en</span><span class="p">)</span>
<span class="n">dictionary_en</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;en.dict&#39;</span><span class="p">)</span>  <span class="c"># save dictionary to file for future use</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span>
<span class="n">dictionary_ko</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">texts_ko</span><span class="p">)</span>
<span class="n">dictionary_ko</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;ko.dict&#39;</span><span class="p">)</span>  <span class="c"># save dictionary to file for future use</span>
</pre></div>


</li>
</ul>
</li>
<li>
<p>Calculate TF-IDF</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">models</span>
<span class="n">tf_en</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary_en</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts_en</span><span class="p">]</span>
<span class="n">tfidf_model_en</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">tf_en</span><span class="p">)</span>
<span class="n">tfidf_en</span> <span class="o">=</span> <span class="n">tfidf_model_en</span><span class="p">[</span><span class="n">tf_en</span><span class="p">]</span>
<span class="n">corpora</span><span class="o">.</span><span class="n">MmCorpus</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="s">&#39;en.mm&#39;</span><span class="p">,</span> <span class="n">tfidf_en</span><span class="p">)</span> <span class="c"># save corpus to file for future use</span>

<span class="c"># print first 10 elements of first document&#39;s tf-idf vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">tfidf_en</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># print top 10 elements of first document&#39;s tf-idf vector</span>
<span class="k">print</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">tfidf_en</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># print token of most frequent element</span>
<span class="k">print</span><span class="p">(</span><span class="n">dictionary_en</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">9</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
[(0, 7), (1, 3), (2, 13), (3, 2), (4, 1), (5, 1), (6, 20), (7, 6), (8, 10), (9, 62)]
[(9, 62), (363, 32), (276, 30), (371, 26), (6, 20), (96, 19), (112, 19), (326, 16), (118, 14), (2, 13)]
'.'
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">models</span>
<span class="n">tf_ko</span> <span class="o">=</span> <span class="p">[</span><span class="n">dictionary_ko</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts_ko</span><span class="p">]</span>
<span class="n">tfidf_model_ko</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">tf_ko</span><span class="p">)</span>
<span class="n">tfidf_ko</span> <span class="o">=</span> <span class="n">tfidf_model_ko</span><span class="p">[</span><span class="n">tf_ko</span><span class="p">]</span>
<span class="n">corpora</span><span class="o">.</span><span class="n">MmCorpus</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="s">&#39;ko.mm&#39;</span><span class="p">,</span> <span class="n">tfidf_ko</span><span class="p">)</span> <span class="c"># save corpus to file for future use</span>

<span class="c"># print first 10 elements of first document&#39;s tf-idf vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">tfidf_ko</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># print top 10 elements of first document&#39;s tf-idf vector</span>
<span class="k">print</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">tfidf_ko</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">])</span>
<span class="c"># print token of most frequent element</span>
<span class="k">print</span><span class="p">(</span><span class="n">dictionary_ko</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="mi">414</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
[(0, 10), (1, 27), (2, 1), (3, 26), (4, 3), (5, 26), (6, 4), (7, 2), (8, 1), (9, 1)]
[(414, 71), (14, 61), (309, 38), (314, 38), (313, 28), (1, 27), (3, 26), (5, 26), (353, 22), (13, 21)]
'하다/Verb'
</pre></p>
</li>
</ul>
</li>
</ol>
<h3>2. Train topic models</h3>
<ol>
<li>
<p>LSI</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">ntopics</span><span class="p">,</span> <span class="n">nwords</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">lsi_en</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">lsimodel</span><span class="o">.</span><span class="n">LsiModel</span><span class="p">(</span><span class="n">tfidf_en</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_en</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lsi_en</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['0.509*"vs" + 0.272*"000" + 0.258*"cts" + 0.243*"loss" + 0.238*"mln"',
'-0.294*"the" + 0.237*"vs" + -0.176*"to" + -0.148*"in" + -0.137*"pct"',
'0.331*"Record" + 0.316*"div" + 0.312*"Pay" + 0.303*"Qtly" + 0.268*"prior"']
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">ntopics</span><span class="p">,</span> <span class="n">nwords</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">lsi_ko</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">lsimodel</span><span class="o">.</span><span class="n">LsiModel</span><span class="p">(</span><span class="n">tfidf_ko</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_ko</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lsi_ko</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['0.518*"육아휴직/Noun" + 0.257*"만/Noun" + 0.227*"×/Foreign" + 0.214*"대체/Noun" + 0.201*"고용/Noun"',
 '0.449*"파견/Noun" + 0.412*"부대/Noun" + 0.267*"UAE/Alpha" + 0.243*"○/Foreign" + 0.192*"국군/Noun"',
 '0.326*"결혼/Noun" + 0.315*"예고/Noun" + 0.285*"손해/Noun" + 0.205*"ㆍ/Foreign" + 0.197*"원사/Noun"']
</pre></p>
</li>
</ul>
</li>
<li>
<p>LDA</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="p">;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c"># optional</span>
<span class="n">lda_en</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span><span class="n">tfidf_en</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_en</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lda_en</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['0.005*the + 0.003*to + 0.003*pct + 0.002*of + 0.002*said',
 '0.005*cts + 0.005*Record + 0.005*div + 0.004*Pay + 0.004*Qtly',
 '0.010*vs + 0.006*mln + 0.006*000 + 0.005*loss + 0.004*cts']
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="p">;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c"># optional</span>
<span class="n">lda_ko</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ldamodel</span><span class="o">.</span><span class="n">LdaModel</span><span class="p">(</span><span class="n">tfidf_ko</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_ko</span><span class="p">,</span> <span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lda_ko</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">num_topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">num_words</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['0.001*학위/Noun + 0.001*파견/Noun + 0.001*손해/Noun + 0.001*간호/Noun + 0.001*소말리아/Noun',
 '0.002*파견/Noun + 0.002*부대/Noun + 0.001*UAE/Alpha + 0.001*손해/Noun + 0.001*○/Foreign',
 '0.003*육아휴직/Noun + 0.002*만/Noun + 0.002*×/Foreign + 0.002*대체/Noun + 0.002*고용/Noun']
</pre></p>
</li>
</ul>
</li>
<li>
<p>HDP</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="p">;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c"># optional</span>
<span class="n">hdp_en</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">hdpmodel</span><span class="o">.</span><span class="n">HdpModel</span><span class="p">(</span><span class="n">tfidf_en</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_en</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hdp_en</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['topic 0: 0.005*the + 0.003*to + 0.002*in + 0.002*a + 0.002*of',
 'topic 1: 0.008*vs + 0.005*000 + 0.004*loss + 0.004*mln + 0.004*cts',
 'topic 2: 0.001*the + 0.001*vs + 0.001*in + 0.001*to + 0.001*mln']
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="p">;</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c"># optional</span>
<span class="n">hdp_ko</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">hdpmodel</span><span class="o">.</span><span class="n">HdpModel</span><span class="p">(</span><span class="n">tfidf_ko</span><span class="p">,</span> <span class="n">id2word</span><span class="o">=</span><span class="n">dictionary_ko</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hdp_ko</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span><span class="n">topics</span><span class="o">=</span><span class="n">ntopics</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">nwords</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
['topic 0: 0.004*소집/Noun + 0.004*도/Josa + 0.004*’/Foreign + 0.004*｢/Foreign + 0.004*9892/Number',
 'topic 1: 0.004*이애주/Noun + 0.004*年/Foreign + 0.004*意思/Foreign + 0.004*마찰/Noun + 0.004*고 려/Noun',
 'topic 2: 0.005*명시/Noun + 0.004*영업정지/Noun + 0.004*세로/Noun + 0.004*중개업/Noun + 0.004*다양하다/Adjective']
</pre></p>
</li>
</ul>
</li>
</ol>
<h3>3. Scoring documents</h3>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">bow</span> <span class="o">=</span> <span class="n">tfidf_model_en</span><span class="p">[</span><span class="n">dictionary_en</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">texts_en</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lsi_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lda_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">hdp_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[(0, 0.1336800876240628),
 (2, -0.030832981664564624),
 (1, -0.39895210562646022)]</p>
<p>[(2, 0.84087091284115845),
 (0, 0.13882114432084294),
 (1, 0.020307942837998694)]</p>
<p>[(0, 0.95369717052959579)]
</pre></p>
<div class="highlight"><pre><span class="n">bow</span> <span class="o">=</span> <span class="n">tfidf_model_en</span><span class="p">[</span><span class="n">dictionary_en</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">texts_en</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lsi_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lda_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">hdp_en</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[(0, 0.072924758682943097),
 (2, -0.0029545572070390153),
 (1, -0.13195370933374836)]</p>
<p>[(0, 0.62957273636869904),
 (2, 0.3270007771486681),
 (1, 0.043426486482632851)]</p>
<p>[(0, 0.90574410236561731),
 (1, 0.010409702375525492)]
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">bow</span> <span class="o">=</span> <span class="n">tfidf_model_ko</span><span class="p">[</span><span class="n">dictionary_ko</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">texts_ko</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lsi_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lda_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">hdp_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[(0, 0.97829017893328929),
 (1, -0.016909513239922121),
 (2, -0.020121561014425089)]</p>
<p>[(2, 0.93880436704581616),
 (0, 0.030626827732744354),
 (1, 0.030568805221439507)]</p>
<p>[(0, 0.94848723192042672),
 (1, 0.014364056233061516),
 (2, 0.010285449586192942)]
</pre></p>
<div class="highlight"><pre><span class="n">bow</span> <span class="o">=</span> <span class="n">tfidf_model_ko</span><span class="p">[</span><span class="n">dictionary_ko</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">texts_ko</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lsi_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">lda_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nb">sorted</span><span class="p">(</span><span class="n">hdp_ko</span><span class="p">[</span><span class="n">bow</span><span class="p">],</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[(0, 0.97829017893328929),
 (1, -0.016909513239922121),
 (2, -0.020121561014425089)]</p>
<p>[(2, 0.93881674048370278),
 (0, 0.0306176131467021),
 (1, 0.030565646369595065)]</p>
<p>[(0, 0.94848723192042672),
 (1, 0.014364056233061516),
 (2, 0.010285449586192942)]
</pre></p>
</li>
</ul>
<h2>Word embedding</h2>
<ul>
<li>Objective: Learn feature vectors from documents<ul>
<li>Text is normally represented with one-hot encoding + hand crafted features</li>
<li>Ex: [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ]</li>
</ul>
</li>
<li><strong>Word embedding</strong>: A set of feature unsupervised learning techniques where words are mapped to n-dimensional vectors of real numbers (the continuous space)<ul>
<li>Use local context to get a more syntactic or semantic representation</li>
<li>Ex: v("cat") = [0.2, -0.4, ..., 0.7], v("mat") = [-0.0, -0.2, ..., -0.1]</li>
</ul>
</li>
<li>Approaches<ul>
<li>Neural networks (Bengio et al., 2001, Mikolov et al., 2013)</li>
<li>Dimensionality reduction (Lebret et al., 2013)</li>
</ul>
</li>
</ul>
<h3>word2vec (Mikolov et al., 2013)</h3>
<ul>
<li>A neural network based embedding method for learning distributed vector representations of words<ul>
<li>No hidden layers!</li>
</ul>
</li>
<li>"an optimized single-machine  can train 100B+ words in one day"</li>
<li>CBOW &amp; Skip-gram: Two ways of creating the "task" for the neural network<br>
    <img src="images/cbow-skip.png" width="600px"></li>
<li>Characteristics<ul>
<li>Places similar words next to each other in a vector space</li>
<li>Places similar relations in parallel (preserve linguistic regularities)<ul>
<li>ex: France: Paris = Germany: Berlin != Italy: Madrid<br>
    <img src="images/countries.png" width="400px"></li>
</ul>
</li>
<li>Linguistic regularities<ul>
<li>v(KING) – v(MAN) + v(WOMAN) = v(QUEEN)</li>
<li>v(KINGS) – v(KING) + v(QUEEN) = v(QUEENS)</li>
<li>v(MADRID) – v(SPAIN) + v(FRANCE) = v(PARIS)</li>
<li><img src="images/regularities.png" width="400px"></li>
</ul>
</li>
</ul>
</li>
<li>Applications<ul>
<li>Machine translation (Socher et al., 2013)<br>
    <img src="images/embedding-mt.png" width="400px"></li>
<li>Jointly embedding images and text (Frome et al., 2013, <a href="http://googleresearch.blogspot.co.uk/2014/11/a-picture-is-worth-thousand-coherent.html">link</a>)<br>
    <img src="images/google-text.png" width="600px"></li>
</ul>
</li>
<li>Some good references to begin with in case you are interested:<ul>
<li>http://radimrehurek.com/2014/02/word2vec-tutorial/</li>
<li>http://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words</li>
</ul>
</li>
</ul>
<p>Let's go for it.</p>
<h3>word2vec toy problem</h3>
<ol>
<li>
<p>Load documents</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">reuters</span>
<span class="n">docs_en</span> <span class="o">=</span> <span class="p">[</span><span class="n">reuters</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">reuters</span><span class="o">.</span><span class="n">fileids</span><span class="p">()]</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.corpus</span> <span class="kn">import</span> <span class="n">kobill</span>
<span class="n">docs_ko</span> <span class="o">=</span> <span class="p">[</span><span class="n">kobill</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kobill</span><span class="o">.</span><span class="n">fileids</span><span class="p">()]</span>
</pre></div>


</li>
</ul>
</li>
<li>
<p>Tokenize</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">texts_en</span> <span class="o">=</span> <span class="n">docs_en</span> <span class="c"># because we loaded tokenized documents in step 1</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">konlpy.tag</span> <span class="kn">import</span> <span class="n">Twitter</span><span class="p">;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Twitter</span><span class="p">()</span>
<span class="n">pos</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="p">[</span><span class="s">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">pos</span><span class="p">(</span><span class="n">d</span><span class="p">)]</span>
<span class="n">texts_ko</span> <span class="o">=</span> <span class="p">[</span><span class="n">pos</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs_ko</span><span class="p">]</span>
</pre></div>


</li>
</ul>
</li>
<li>
<p>Train</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>
<span class="n">wv_model_en</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">texts_en</span><span class="p">)</span>
<span class="n">wv_model_en</span><span class="o">.</span><span class="n">init_sims</span><span class="p">(</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">wv_model_en</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;en_word2vec.model&#39;</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">word2vec</span>
<span class="n">wv_model_ko</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">texts_ko</span><span class="p">)</span>
<span class="n">wv_model_ko</span><span class="o">.</span><span class="n">init_sims</span><span class="p">(</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">wv_model_ko</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;ko_word2vec.model&#39;</span><span class="p">)</span>
</pre></div>


</li>
</ul>
</li>
<li>
<p>Test</p>
<ul>
<li>
<p>English</p>
<div class="highlight"><pre><span class="n">wv_model_en</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">&#39;president&#39;</span><span class="p">)</span>
<span class="n">wv_model_en</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">&#39;secretary&#39;</span><span class="p">)</span>
<span class="n">wv_model_en</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">&#39;country&#39;</span><span class="p">)</span>
</pre></div>


<p><pre class="result">
[('chairman', 0.8655247688293457),
 ('vice', 0.8160154819488525),
 ('executive', 0.8094440698623657),
 ('officer', 0.7894954085350037),
 ('Kjell', 0.7766541838645935),
 ('former', 0.7680522203445435),
 ('chief', 0.7660256028175354),
 ('Robert', 0.7623487114906311),
 ('director', 0.7434573173522949),
 ('Roger', 0.7231118679046631)]</p>
<p>[('assistant', 0.8573123812675476),
 ('Carlos', 0.796258807182312),
 ('Daniel', 0.7900130748748779),
 ('undersecretary', 0.7888025045394897),
 ('representative', 0.7878221273422241),
 ('Deputy', 0.7847912311553955),
 ('NAWG', 0.7829214930534363),
 ('Republican', 0.7773356437683105),
 ('Greek', 0.7752739191055298),
 ('Papandreou', 0.7684933543205261)]</p>
<p>[('kingdom', 0.8003361225128174),
 ('biggest', 0.765742301940918),
 ('island', 0.7639101147651672),
 ('founding', 0.7143765687942505),
 ('nation', 0.7080289125442505),
 ('fortunes', 0.7054018974304199),
 ('strength', 0.6875098943710327),
 ('challenging', 0.6863174438476562),
 ('actions', 0.6835225820541382),
 ('departure', 0.6834459900856018)]
</pre></p>
</li>
<li>
<p>Korean</p>
<div class="highlight"><pre><span class="n">wv_model_ko</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">pos</span><span class="p">(</span><span class="s">&#39;정부&#39;</span><span class="p">))</span>
<span class="n">wv_model_ko</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">pos</span><span class="p">(</span><span class="s">&#39;초등학교&#39;</span><span class="p">))</span>
</pre></div>


<p><pre class="result">
[('경비/Noun', 0.9357226490974426),
 ('선박/Noun', 0.9204540252685547),
 ('연장/Noun', 0.9183653593063354),
 ('임무/Noun', 0.9179578423500061),
 ('우리/Noun', 0.9015840291976929),
 ('목적/Noun', 0.8871368169784546),
 ('기타/Noun', 0.875058650970459),
 ('화/Suffix', 0.8669425249099731),
 ('해역/Noun', 0.8575668334960938),
 ('한국/Noun', 0.8549510836601257)]</p>
<p>[('취학/Noun', 0.9686248898506165),
 ('중인/Noun', 0.9336546659469604),
 ('하더/Verb', 0.8985729217529297),
 ('정의화/Noun', 0.8843945860862732),
 ('김정훈/Noun', 0.8682949542999268),
 ('지방/Noun', 0.8677719831466675),
 ('조정함/Verb', 0.8617256879806519),
 ('44/Number', 0.8445801734924316),
 ('세/Noun', 0.8318654298782349),
 ('第/Foreign', 0.8222816586494446)]
</pre></p>
</li>
</ul>
</li>
</ol>
<h3>word2vec in the real world</h3>
<p>Not enough? Let's see a real life example.</p>
<ul>
<li>Data source: Naver News &amp; Naver blog<br>
    <img src="images/experiment.png" width="500px"></li>
<li>Questions<br>
    <img src="images/questions.png" width="500px"></li>
<li>Matching pairs: 그/Noun:남자/Noun = 그녀/Noun:?<br>
    <img src="images/pairs.png" width="500px"></li>
<li>Visualization<br>
    <img src="images/tsne1.png" width="600px"><br>
    <img src="images/tsne2.png" width="600px"><br>
    <img src="images/tsne3.png" width="600px"><br></li>
</ul>
<!--

## Text classification
### Sentiment analysis
- https://github.com/nltk/nltk/wiki/Sentiment-Analysis

## Machine translation
- http://www.statmt.org/

## Deep learning
- https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software#senna-for-various-nlp-tasks
    - http://ml.nec-labs.com/senna/
-->

    </div>
    <div class="footnote">
        <div class="social-links pull-right">
            <div id="facebook-like">
                <div id="fb-root"></div>
                <script>(function(d, s, id) {
                  var js, fjs = d.getElementsByTagName(s)[0];
                  if (d.getElementById(id)) return;
                  js = d.createElement(s); js.id = id;
                  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=357567114353849";
                  fjs.parentNode.insertBefore(js, fjs);
                }(document, 'script', 'facebook-jssdk'));</script>
                <div class="fb-like" data-href="http://lucypark.kr/courses/2015-ba/text-mining.html" data-send="false" data-layout="button_count" data-width="450" data-show-faces="false"></div>
            </div>
            <div id="twitter-tweet">
                <a href="https://twitter.com/share" class="twitter-share-button" data-lang="en"></a>
                <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="https://platform.twitter.com/widgets.js";fjs.parentNode.    insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
            </div>
        </div>
        <p>
            Author:
            <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                <a href="http://dm.snu.ac.kr/~epark"><span itemprop="name">Lucy Park</span></a>
            </span>
        </p>
        <p>
            Category:
            <span itemprop="articleSection">
                <a href="http://lucypark.kr/courses/category/2015-ba.html" rel="category">2015-ba</a>
            </span>
        </p>
        <p>
            Tags:
            <span itemprop="keywords">
                <a href="http://lucypark.kr/courses/tag/text.html" rel="tag">text</a>
            </span>
            <span itemprop="keywords">
                <a href="http://lucypark.kr/courses/tag/lectures.html" rel="tag">lectures</a>
            </span>
        </p>
    </div>
</div>
		</div>
	</div> 	<!-- <hr> -->
</div> <!-- /container -->
<footer class="aw-footer bg-danger">
	<div class="container"> <!-- footer -->
		<div class="center-block aw-bottom">
            Built by <a href="http://dm.snu.ac.kr/~epark">Lucy Park</a>.<br>
            Code licensed under <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License v2.0</a>, document under <a href="http://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.<br>
 The code for this site is located at <a href="http://github.com/e9t/courses">GitHub</a>.             <span class="pull-right margin-top-30">
                <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a>
                <a href="http://lucypark.kr/courses/feeds/all.atom.xml" type="application/atom+xml"><i class="fa fa-rss "></i> atom</a>
            </span>
		</div>
	</div>
</footer>
<div class="container">
	<div class="row">
	</div>
</div>
<!-- JavaScript -->
<script src="//code.jquery.com/jquery-2.1.1.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
<script type="text/javascript">
jQuery(document).ready(function($) {
	$("div.collapseheader").click(function () {
		$header = $(this).children("span").first();
		$codearea = $(this).children(".input_area");
		$codearea.slideToggle(500, function () {
			$header.text(function () {
				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
			});
		});
	});
});

// add 'active' class to current menu
$(function(){
  if (document.location.hostname == "localhost") {
    var idx = 1;
    var activePage = location.pathname.split("/")[idx];
  } else {
    var idx = 4;
    var activePage = location.href.split("/")[idx];
  }
  $('.nav li a').each(function() {
    var currentPage = $(this).attr('href').split("/")[idx];
    if (activePage == currentPage) {
      $(this).parent().addClass('active');
    }
  });
});
</script>
<script>
    function hyphenate(str) {
        return str.toLowerCase().replace(/[?!:'"\[\]()%.,]/gi, '').replace(/\s/g, '-');
    }

    String.prototype.repeat = function(num) { return new Array(num + 1).join(this); }

    // create toc
    var t = '';
    var concat = '';
    var headers = $('.article-body :header');
    headers.each(function() {
        var hyphenated = hyphenate($(this).text());
            $(this).attr('id', hyphenated);
        }
    );
    for (var i=0; i<headers.length; i++) {
        depth = parseInt(headers[i].tagName.substr(-1));
        if (depth < 4) {
            t = headers[i].textContent;
            concat += '&nbsp;&nbsp;&nbsp;&nbsp;'.repeat(parseInt(depth)-2) + '- <a href="#' + hyphenate(t) + '">' + t + '</a><br>';
        }
    }
    $(".article-toc p").append(concat);

    // scroll animation
    $(document).ready(function(){
        $('a[href^="#"]').on('click',function (e) {
            e.preventDefault();

            var target = this.hash;
            var $target = $(target);

            $('html, body').stop().animate({
                'scrollTop': $target.offset().top
            }, 300, 'swing', function () {
                window.location.hash = target;
            });
        });
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$', '$$'], ["\\[", "\\]"]],
    }});
</script>
<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', UA-4510606-3, 'aboutwilson.net');
ga('require', 'displayfeatures');
ga('require', 'linkid', 'linkid.js');
ga('send', 'pageview');

</script>
</body>
</html>